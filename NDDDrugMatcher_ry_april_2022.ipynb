{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBfzXumrErR7"
      },
      "source": [
        "To Due - Still ( April 2022)\n",
        "\n",
        "Check other counties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Might want to look in to grabbing non-us studies\n",
        "\n",
        "https://www.clinicaltrials.gov/ct2/resources/trends\n",
        "\n",
        "![alt text](https://www.clinicaltrials.gov/ct2/resources/trends/location-count-pie \"Title\")\n",
        "Location\tNumber of Registered Studies and Percentage of Total\n",
        "(as of March 27, 2022)\n",
        "\n",
        "Non-U.S. only\t211,870   (52%)\n",
        "U.S. only\t130,667   (32%)\n",
        "Both U.S. and non-U.S.\t20,450   (5%)\n",
        "Not provided\t46,133   (11%)\n",
        "Total\t409,120   (100%)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghbnG1oG_eig"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0tJviEpp37q"
      },
      "source": [
        "## dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I1naMwBgp1Ok"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import rcParams, cycler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import AutoMinorLocator\n",
        "import csv\n",
        "import json #For JSON Export/Imports\n",
        "import os \n",
        "import wget\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbGSVyhl3ePD",
        "outputId": "01a90216-ccfe-404b-9884-6d0099a2b24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11:42:08\n"
          ]
        }
      ],
      "source": [
        "  \n",
        "curr_time = time.localtime()\n",
        "curr_clock = time.strftime(\"%H:%M:%S\", curr_time)\n",
        "  \n",
        "print(curr_clock)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iejy7w4-EZIX"
      },
      "source": [
        "## Directory and classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## classifers \n",
        "url10 = 'https://www.dropbox.com/s/uo4h5uy6fe7texr/chartsv4a_ry.csv?dl=1'\n",
        "file10 = 'behaviors-class.txt'\n",
        "if not os.path.exists(file10):\n",
        "    filename = wget.download(url10)\n",
        "\n",
        "url11 = 'https://www.dropbox.com/s/h9f82j5s1llckk1/chartsv4B_ry.csv?dl=1'\n",
        "file11 = 'biomarkers-class.txt'\n",
        "if not os.path.exists(file11):\n",
        "    filename = wget.download(url11)\n",
        "\n",
        "url12 = 'https://www.dropbox.com/s/cz2z58ips1hapx1/chartsv4C_ry.csv?dl=1'\n",
        "file12 = 'deleteList-class.txt'\n",
        "if not os.path.exists(file12):\n",
        "    filename = wget.download(url12)\n",
        "\n",
        "url13 = 'https://www.dropbox.com/s/7wtstfey6u5gq5c/chartsv4D_ry.csv?dl=1'\n",
        "file13 = 'devices-class.txt'\n",
        "if not os.path.exists(file13):\n",
        "    filename = wget.download(url13)\n",
        "\n",
        "url14 = 'https://www.dropbox.com/s/7wtstfey6u5gq5c/chartsv4D_ry.csv?dl=1'\n",
        "file14 = 'dm-subclass.txt'\n",
        "if not os.path.exists(file14):\n",
        "    filename = wget.download(url14)\n",
        "\n",
        "url15 = 'https://www.dropbox.com/s/7wtstfey6u5gq5c/chartsv4D_ry.csv?dl=1'\n",
        "file15 = 'drugs-class.txt'\n",
        "if not os.path.exists(file15):\n",
        "    filename = wget.download(url15)\n",
        "\n",
        "url16 = 'https://www.dropbox.com/s/uo4h5uy6fe7texr/chartsv4a_ry.csv?dl=1'\n",
        "file16 = 'eligCritExcl.txt'\n",
        "if not os.path.exists(file16):\n",
        "    filename = wget.download(url16)\n",
        "\n",
        "url17 = 'https://www.dropbox.com/s/h9f82j5s1llckk1/chartsv4B_ry.csv?dl=1'\n",
        "file17 = 'stemcells-class.txt'\n",
        "if not os.path.exists(file17):\n",
        "    filename = wget.download(url17)\n",
        "\n",
        "url18 = 'https://www.dropbox.com/s/cz2z58ips1hapx1/chartsv4C_ry.csv?dl=1'\n",
        "file18 = 'supplement-class.txt'\n",
        "if not os.path.exists(file18):\n",
        "    filename = wget.download(url18)\n",
        "\n",
        "url19 = 'https://www.dropbox.com/s/7wtstfey6u5gq5c/chartsv4D_ry.csv?dl=1'\n",
        "file19 = 'chartsv4D_ry.csv'\n",
        "if not os.path.exists(file19):\n",
        "    filename = wget.download(url19)\n",
        "\n",
        "url20 = 'https://www.dropbox.com/s/cz2z58ips1hapx1/chartsv4C_ry.csv?dl=1'\n",
        "file20 = 'sx-subclass.txt'\n",
        "if not os.path.exists(file20):\n",
        "    filename = wget.download(url20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvadhI3Q260-",
        "outputId": "12ceb766-5319-4c07-c5b4-cbae076aa14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ric/Desktop/ndd_redo/ndd_notebook\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#updated Links for 2022\n",
        "Updated to pull csv \n",
        "Takes longer but don't have to unzip / 7z them \n",
        "also alloicates the size ( were if you download and then unzip you might run out of space)\n",
        "\n",
        "--changed run time from 30 second to 3 mins and 30 sec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1qT1zOTErQ4H"
      },
      "outputs": [],
      "source": [
        "## need to break appart file and URL or something so I don't have to download files each time\n",
        "\n",
        "url = 'https://www.dropbox.com/s/uo4h5uy6fe7texr/chartsv4a_ry.csv?dl=1'\n",
        "file = 'chartsv4a_ry.csv'\n",
        "if not os.path.exists(file):\n",
        "    filename = wget.download(url)\n",
        "\n",
        "url2 = 'https://www.dropbox.com/s/h9f82j5s1llckk1/chartsv4B_ry.csv?dl=1'\n",
        "file2 = 'chartsv4B_ry.csv'\n",
        "if not os.path.exists(file2):\n",
        "    filename = wget.download(url2)\n",
        "\n",
        "url3 = 'https://www.dropbox.com/s/cz2z58ips1hapx1/chartsv4C_ry.csv?dl=1'\n",
        "file3 = 'chartsv4C_ry.csv'\n",
        "if not os.path.exists(file3):\n",
        "    filename = wget.download(url3)\n",
        "\n",
        "url4 = 'https://www.dropbox.com/s/7wtstfey6u5gq5c/chartsv4D_ry.csv?dl=1'\n",
        "file4 = 'chartsv4D_ry.csv'\n",
        "if not os.path.exists(file4):\n",
        "    filename = wget.download(url4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6ONqSCr-7Rtt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chartsv4a_ry.csv -> NDDDrugMatcher/queries/chartsv4A.csv\n",
            "chartsv4B_ry.csv -> NDDDrugMatcher/queries/chartsv4B.csv\n",
            "chartsv4C_ry.csv -> NDDDrugMatcher/queries/chartsv4C.csv\n",
            "chartsv4D_ry.csv -> NDDDrugMatcher/queries/chartsv4D.csv\n"
          ]
        }
      ],
      "source": [
        "!mv -fv chartsv4a_ry.csv    NDDDrugMatcher/queries/chartsv4A.csv\n",
        "!mv -fv chartsv4B_ry.csv  NDDDrugMatcher/queries/chartsv4B.csv\n",
        "!mv -fv chartsv4C_ry.csv    NDDDrugMatcher/queries/chartsv4C.csv\n",
        "!mv -fv chartsv4D_ry.csv    NDDDrugMatcher/queries/chartsv4D.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LylKuWfxj0HO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/queries/'):\n",
        "    os.makedirs('NDDDrugMatcher/queries/')\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/'):\n",
        "    os.makedirs('NDDDrugMatcher/output/')\n",
        "\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/classified-tables'):\n",
        "    os.makedirs('NDDDrugMatcher/output/classified-tables')\n",
        "\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/json-tables/'):\n",
        "    os.makedirs('NDDDrugMatcher/output/json-tables/')\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/json-tables/hyperlinked/'):\n",
        "    os.makedirs('NDDDrugMatcher/output/json-tables/hyperlinked/')\n",
        "\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/json-tables/hyperlinked/'):\n",
        "    os.makedirs('NDDDrugMatcher/json-tables/hyperlinked/')\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/final-tables/hyperlinked/'):\n",
        "    os.makedirs('NDDDrugMatcher/output/final-tables/hyperlinked/')\n",
        "\n",
        "\n",
        "if not os.path.exists('NDDDrugMatcher/output/eligibility-criteria/'):\n",
        "    os.makedirs('NDDDrugMatcher/output/eligibility-criteria/')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "else: \n",
        "  print('done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVjWzFtxAsWm",
        "outputId": "69cfb9be-7e29-4a53-beca-a2caeba9fa4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md            drugmatcherv6.py     nddfilter.py\n",
            "common.py            eligcritprocesser.py \u001b[1m\u001b[36moutput\u001b[m\u001b[m\n",
            "\u001b[1m\u001b[36mcsvClassifier\u001b[m\u001b[m        \u001b[1m\u001b[36minput\u001b[m\u001b[m                \u001b[1m\u001b[36mqueries\u001b[m\u001b[m\n",
            "drugclassifier.py    \u001b[1m\u001b[36mjson-tables\u001b[m\u001b[m          tablegeneration.py\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# os.chdir('NDDDrugMatcher/')\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uxeEdqS0prb"
      },
      "source": [
        "##common py\n",
        "\n",
        "Soo break out the disease stuff and see if I can some how look for related terms ( like mesh terms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Functions / Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class jfc:\n",
        "    #checks / searchs for arconyms\n",
        "    diseaseAcronymsChecked = {\n",
        "                              # \"AD\",\n",
        "                              # \"ALS\", \n",
        "                              \"CBD\", \n",
        "                              \"CTE\", \n",
        "                              # \"LBD\", \n",
        "                              \"DLB\", \n",
        "                              \"LWB\", \n",
        "                              # \"FTDLD\", \n",
        "                              # \"HD\", \n",
        "                              # \"MSA\", \n",
        "                              \"PD\", \n",
        "                              # \"PSP\", \n",
        "                              \"MCI\"}\n",
        "\n",
        "# checks / searchs for full nam,e\n",
        "    diseaseLongChecked = {\n",
        "                      # \"Alzh\",\n",
        "                      #     \"Amyotro\",\n",
        "                      #     \"Chronic Traumatic Encephalopathy\",\n",
        "                      #     \"Corticobasal Degen\",\n",
        "                      \"Dementia With Lewy Bodies\",\n",
        "                      \"Frontotemporal\",\n",
        "                      # \"Huntington\",\n",
        "                      # \"Multiple System Atrophy\",\n",
        "                      \"Parkin\",\n",
        "                      # \"Progressive Supranuclear Palsy\",\n",
        "                       \"Mild Cognitive Impairment\"}\n",
        "\n",
        "\n",
        "    #To try partial matching\n",
        "    diseaselowercase = {\n",
        "                      # \"alzh\",\n",
        "                      #   \"amyotro\",\n",
        "                      #   \"chronic traumatic encephalopathy\",\n",
        "                      #   \"corticobasal degen\",\n",
        "                      # \"dementia with lewy bodies\",\n",
        "                      # \"frontotemporal\",\"huntington\",\n",
        "                      # \"multiple system atrophy\",\n",
        "                      \"parkin\",\n",
        "                      # \"progressive supranuclear palsy\", \n",
        "                      # \"mild cognitive impairment\", \n",
        "                      # \"palsy\",\n",
        "                       \" pd \",\n",
        "                      # \" msa \", \n",
        "                      # \" ad \", \n",
        "                      # \"ftdld\", \n",
        "                      # \" dlb \",\n",
        "                       \"lewy body dementia\", \n",
        "                        # \"(pd,\", \n",
        "                        \"lewy body disease\",\n",
        "                      # \"multiple systems atrophy\",\n",
        "                       \"lewy body dementia\"}\n",
        "\n",
        "    #So this one is so we can see if diseases are different by using the power of hashing\n",
        "    #only meant for duplicate removal atm\n",
        "    diseaseHashMap = {\n",
        "        # \"AD\": \"AD\",\n",
        "        \" pd \": \"PD\",\n",
        "        \" ad \": \"AD\",\n",
        "        # \"ALS\": \"ALS\",\n",
        "        # \"CBD\": \"CBD\",\n",
        "        # \"CTE\": \"CTE\",\n",
        "        # \"DLB\": \"DLB\",\n",
        "        \" dlb \": \"DLB\",\n",
        "        \"LWB\": \"LWB\",\n",
        "        # \"FTDLD\": \"FTDLD\",\n",
        "        # \"ftdld\": \"FTDLD\",\n",
        "        # \"HD\": \"HD\",\n",
        "        # \"MSA\": \"MSA\",\n",
        "        # \" msa \": \"MSA\",\n",
        "        \"PD\": \"PD\",\n",
        "        \"(pd,\": \"PD\",\n",
        "        # \"PSP\": \"PSP\",\n",
        "        \"MCI\": \"MCI\",\n",
        "        # \"Alzh\": \"AD\",\n",
        "        # \"alzh\": \"AD\",\n",
        "        # \"ALZ\" : \"AD\", #TODO, use to lower in getCleanCondition....\n",
        "        # \"Amyotro\": \"ALS\",\n",
        "        # \"amyotro\": \"ALS\",\n",
        "        # \"Alzheimer's Disease\": \"AD\",\n",
        "        # \"Corticobasal Degeneration\": \"CBD\",\n",
        "        # \"corticobasal degen\": \"CBD\",\n",
        "        # #\"Lewy Body Disease\": \"LWB\",\n",
        "        # \"Frontotemporal Lobar Degeneration\": \"FTDLD\",\n",
        "        # \"Huntington's Disease\": \"HD\",\n",
        "        # \"huntington\": \"HD\",\n",
        "        \"Parkinson's Disease\": \"PD\",\n",
        "        # \"Amyotrophic Lateral Sclerosis\": \"ALS\",\n",
        "        # \"AMYOTROPHIC LATERAL SCLEROSIS\": \"ALS\",\n",
        "        # \"Chronic Traumatic Encephalopathy\": \"CTE\",\n",
        "        # \"chronic traumatic encephalopathy\": \"CTE\",\n",
        "        # \"Encephalo\": \"CTE\",\n",
        "        # \"Corticobasal Degen\": \"CTE\",\n",
        "        # \"Cortico\": \"CTE\",\n",
        "        \"Dementia With Lewy Bodies\": \"DLB\",\n",
        "        \"Dementia with Lewy Bodies\": \"DLB\",\n",
        "        \"dementia with lewy bodies\": \"DLB\",\n",
        "        \"Lewy Body Dementia\": \"DLB\",\n",
        "        \"lewy body dementia\": \"DLB\",\n",
        "        \"Dementia, Lewy Body\": \"DLB\",\n",
        "        \"Lewy Body Disease\": \"DLB\",\n",
        "        \"lewy body disease\": \"DLB\",\n",
        "        # \"Frontotemporal\": \"FTDLD\",\n",
        "        # \"frontotemporal\": \"FTDLD\",\n",
        "        # \"Fronto\": \"FTDLD\",\n",
        "        # \"Huntington\": \"HD\",\n",
        "        # \"Multiple System Atrophy\": \"MSA\",\n",
        "        # \"multiple system atrophy\": \"MSA\",\n",
        "        # \"Multiple Systems Atrophy\": \"MSA\",\n",
        "        # \"multiple systems atrophy\": \"MSA\",\n",
        "        \"Parkin\": \"PD\",\n",
        "        \"parkin\": \"PD\",\n",
        "        \"PARKIN\": \"PD\",\n",
        "        # \"Progressive Supranuclear Palsy\": \"PSP\",\n",
        "        # \"progressive supranuclear palsy\": \"PSP\",\n",
        "        # \"Supranuclear Palsy, Progressive\": \"PSP\",\n",
        "        # \"supranuclear palsy\": \"PSP\",\n",
        "        # \"Palsy\": \"PSP\",\n",
        "        # \"palsy\": \"PSP\",\n",
        "        \"Parkinson\": \"PD\",\n",
        "        \"Mild Cognitive Impairment\": \"MCI\",\n",
        "        \"MILD COGNITIVE IMPAIRMENT\": \"MCI\",\n",
        "        \"Mild CognitIve Impairment\": \"MCI\",\n",
        "        \"mild cognitive impairment\": \"MCI\",\n",
        "        \"Healthy Volunteers\": \" \" #could also do \"HV\"\n",
        "    }\n",
        "\n",
        "\n",
        "    diseaseinTitleChecked = {\n",
        "        \n",
        "                    # \"Chronic Traumatic Encephalopathy\",\n",
        "                    #          \"Corticobasal Degen\",\n",
        "                    #          \"Dementia With Lewy Bodies\",\n",
        "                        #  \"Frontotemporal\",\n",
        "                        #  \"Huntington\",\n",
        "                        #  \"Multiple System Atrophy\",\n",
        "                         \"Parkinson\",\n",
        "                        #  \"Progressive Supranuclear Palsy\", \n",
        "                         \"Mild Cognitive Impairment\"}\n",
        "\n",
        "    #New Disease Hash Map for getCleanCondition\n",
        "    #\"AD\", \"ALS\", \"CBD\", \"CTE\", \"DLB\", \"LWB\", \"FTDLD\", \"HD\", \"MSA\", \"PD\", \"PSP\"\n",
        "    diseaseAcronymFullNameMap = {\n",
        "        # \"AD\": \"Alzheimer's Disease\",\n",
        "        # \"ALS\": \"Amyotrophic Lateral Sclerosis\",\n",
        "        # \"CBD\": \"Corticobasal Degeneration\",\n",
        "        # \"CTE\": \"Chronic Traumatic Encephalopathy\",\n",
        "        \"DLB\": \"Dementia With Lewy Bodies\",\n",
        "        \"LBD\": \"Dementia With Lewy Bodies\",\n",
        "        \"LWB\": \"Lewy Body Disease\",\n",
        "        # \"FTDLD\": \"Frontotemporal Lobar Degeneration\",\n",
        "        # \"HD\": \"Huntington's Disease\",\n",
        "        # \"MSA\": \"Multiple System Atrophy\",\n",
        "        \"PD\": \"Parkinson's Disease\",\n",
        "        # \"PSP\": \"Progressive Supranuclear Palsy\",\n",
        "        \"MCI\": \"Mild Cognitive Impairment\"\n",
        "    }\n",
        "\n",
        "    diseaseFullNames = { #Set with full name of diseases for string matching\n",
        "        # \"Alzheimer's Disease\",\n",
        "        # \"Amyotrophic Lateral Sclerosis\",\n",
        "        # \"Corticobasal Degeneration\",\n",
        "        # \"Chronic Traumatic Encephalopathy\",\n",
        "        \"Dementia With Lewy Bodies\",\n",
        "        \"Lewy Body Disease\",\n",
        "        # \"Frontotemporal Lobar Degeneration\",\n",
        "        # \"Huntington's Disease\",\n",
        "        # \"Multiple System Atrophy\",\n",
        "        \"Parkinson's Disease\",\n",
        "        # \"Progressive Supranuclear Palsy\",\n",
        "        \"Mild Cognitive Impairment\",\n",
        "        \"Healthy Volunteers\"\n",
        "    }\n",
        "    @staticmethod #Function that checks if the drugname is ok to add (currently removed placebo, but we can add stopwords here)\n",
        "    def drugnameCheckOK(drugname): #Also going to check for control since that is control group\n",
        "        dname = drugname.lower()\n",
        "        if \"placebo\" not in dname and \"control\" not in dname: #If there is placebo we want to return false\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    #Function that returns clean condition name for comparison. NEW FUNCTION \n",
        "    @staticmethod\n",
        "    def getCleanCondition(cond):\n",
        "        cleanCond = cond\n",
        "        if cleanCond in jfc.diseaseAcronymsChecked:\n",
        "            cleanCond = jfc.diseaseAcronymFullNameMap[cleanCond] #Standard Acronym so we're fine\n",
        "            return cleanCond #split into two lines for debugging\n",
        "        elif cleanCond in jfc.diseaseFullNames:\n",
        "            return cleanCond #perfect match!\n",
        "        else:\n",
        "            for entry in jfc.diseaseHashMap:\n",
        "                if entry in cleanCond:\n",
        "                    cleanCond = jfc.diseaseAcronymFullNameMap[jfc.diseaseHashMap[entry]]\n",
        "                    return cleanCond\n",
        "        #fi we reach here the for loop failed to find something so we need to see what it was!\n",
        "        print(\"WARNING: Unable to Identify Condition:\", cond)\n",
        "        return cleanCond\n",
        "\n",
        "    #Class to more cleanly store the trials (which we will then convert to JSON for the App)\n",
        "    class clinicalTrial:\n",
        "        def __init__(self, nctid, title, condition, intervention, intervention_type, otherIntervention, lastpostedDate, start_date,completion_date,why_stopped, \n",
        "                    phase, acronym, enrollment, timeFrame, firstpostedDate,\n",
        "                    studyStatus, flexibleCondition=False): #outcomeTitle #outcomeDescription\n",
        "            self.nctid = nctid\n",
        "            self.title = title\n",
        "            self.condition = []\n",
        "            self.intervention = []\n",
        "    #added because of larger pull\n",
        "            self.intervention_type = intervention_type\n",
        "            self.otherIntervention = []\n",
        "            self.lastpostedDate = lastpostedDate\n",
        "    #added because of larger pull\n",
        "            self.start_date = start_date\n",
        "            self.completion_date = completion_date\n",
        "            self.why_stopped= why_stopped\n",
        "\n",
        "\n",
        "            self.phase = phase #phase.replace(\"Phase \", \"P\") #for our table appearance\n",
        "            self.acronym = acronym\n",
        "            self.enrollment = enrollment\n",
        "            self.timeFrame = timeFrame\n",
        "            self.designTimeFrame = \"\" #We will add this later\n",
        "            self.designMeasures = [] #We will add this later\n",
        "            self.designDescription = [] #We will add this later\n",
        "            self.firstpostedDate = firstpostedDate\n",
        "            self.studyStatus = studyStatus\n",
        "            self.outcomeTitle = [] #we will add these later\n",
        "            self.outcomeDescription = [] #we will add these later\n",
        "            self.eligibilityCriteria = [] #we optionally can add these later\n",
        "            self.nddInEligCriteria = [] #we optionally can add these later, if any, only stores NDD that dont appear already in conditions\n",
        "            #By default we want warnings if condition isn't found in hashmap but can disable for non NDD Trials. So we used default\n",
        "            self.flexibleCondition = flexibleCondition #paramater set to false so can only actively disable the check\n",
        "            self.addCondition(condition)\n",
        "            self.addInterventions(intervention, 0)\n",
        "            self.addInterventions(otherIntervention, 1)\n",
        "\n",
        "\n",
        "        def addCondition(self, cond):\n",
        "            #Todo, clean up condition by storing under one clean name so like no mispellings, also dont store dups\n",
        "            if not self.flexibleCondition:\n",
        "                cleanCond = jfc.getCleanCondition(cond)\n",
        "            else: #Flexible condition, throw it in do no checks. Yolo\n",
        "                cleanCond = cond\n",
        "            if cleanCond not in self.condition:\n",
        "                self.condition.append(cleanCond)\n",
        "        def addInterventions(self, intervention, intType):\n",
        "            if intervention == \"\":\n",
        "                return #don't add empty stuff\n",
        "            if intType == 0: #intervention\n",
        "                if intervention not in self.intervention:\n",
        "                    self.intervention.append(intervention) #to avoid duplicates\n",
        "            else: #other intervention\n",
        "                if intervention not in self.otherIntervention:\n",
        "                    self.otherIntervention.append(intervention)\n",
        "        def setFlexibleCondition(self): #Enables flexibility in condition, will disable warning\n",
        "            self.flexibleCondition = True\n",
        "        def addOutcome(self, oT, oD):\n",
        "            if oT not in self.outcomeTitle:\n",
        "                self.outcomeTitle.append(oT)\n",
        "            if oD not in self.outcomeDescription:\n",
        "                self.outcomeDescription.append(oD)\n",
        "        def addDesignOutcomes(self, dMeasure, dTimeFrame, dDescription):\n",
        "            self.designTimeFrame = dTimeFrame\n",
        "            if dMeasure not in self.designMeasures:\n",
        "                self.designMeasures.append(dMeasure)\n",
        "            if dDescription not in self.designDescription:\n",
        "                self.designDescription.append(dDescription)\n",
        "        def addEligibilityCriteria(self, e):\n",
        "            if e not in self.eligibilityCriteria:\n",
        "                self.eligibilityCriteria.append(e)\n",
        "        def addNDDInEligCriteria(self, nddlist): #Note this only stores clean version, aka not NDD that appear in cond already\n",
        "            if isinstance(nddlist, list):\n",
        "                for ndd in nddlist: #list of them try to insert one at a time\n",
        "                    if ndd not in self.nddInEligCriteria:\n",
        "                        self.nddInEligCriteria.append(ndd)\n",
        "            else: #not a list, single NDD so try to insert it directly\n",
        "                if nddlist not in self.nddInEligCriteria:\n",
        "                    self.nddInEligCriteria.append(nddlist)\n",
        "        def getShortPhase(self):\n",
        "            return self.phase.replace(\"Phase \", \"P\") #for our table appearance\n",
        "        def getShortTimeFrame(self):\n",
        "            if self.timeFrame != \"\":\n",
        "                shortend = self.timeFrame\n",
        "            else:\n",
        "                shortend = self.designTimeFrame\n",
        "            shortend = shortend.replace(\"months\", \"m\")\n",
        "            shortend = shortend.replace(\"days\", \"d\")\n",
        "            shortend = shortend.replace(\"weeks\", \"w\")\n",
        "            return shortend\n",
        "        def getConditionAcronyms(self): #Returns acronyms of all conditions here as a list\n",
        "            if(self.flexibleCondition): #If flexible condition is enabled we can't generate acronyms\n",
        "                return self.condition #so just return the raw condition list.\n",
        "            cond = []\n",
        "            for i in self.condition:\n",
        "                #for x in jfc.diseaseHashMap: #for when we dont trust if there are errors\n",
        "                #    if x in i:\n",
        "                #        cleanCond = x\n",
        "                #        break\n",
        "                #cond.append(jfc.diseaseHashMap[cleanCond])\n",
        "                cond.append(jfc.diseaseHashMap[i]) #For when there are no errors   (We ought to be ok)\n",
        "            return cond\n",
        "        def getConditionAcronymsStr(self): #Returns acronyms of all conditions here as a list\n",
        "            condlist = self.getConditionAcronyms()\n",
        "            finalstring = \"\"\n",
        "            if len(condlist) > 0:\n",
        "                finalstring = condlist[0]\n",
        "                if len(condlist) > 1:\n",
        "                    for i in range(1,len(condlist)):\n",
        "                        finalstring += \", \" + condlist[i] #for comma to look nice\n",
        "            return finalstring\n",
        "        def getNDDInEligCriteriaStr(self): #Returns string version of nddInEligCriteria\n",
        "            finalstring = \"\"\n",
        "            if len(self.nddInEligCriteria) > 0:\n",
        "                finalstring = self.nddInEligCriteria[0]\n",
        "                if len(self.nddInEligCriteria) > 1:\n",
        "                    for i in range(1, len(self.nddInEligCriteria)):\n",
        "                        finalstring += \", \" + self.nddInEligCriteria[i] #for comma to look nice\n",
        "            return finalstring\n",
        "        def getComboStatus(self): #Function that returns the fancy YEAR; Status from table\n",
        "            return str(self.getOnlyDateYear(self.lastpostedDate)) + \"; \" + self.studyStatus\n",
        "        def getTablePrimaryOutcomeStr(self): #makes string with the outcome titles separated by a ;\n",
        "            finalstr = \"\"\n",
        "            if len(self.outcomeTitle) > 0:\n",
        "                finalstr = self.outcomeTitle[0]\n",
        "                if len(self.outcomeTitle) > 1: #nasty logic to get ; in right spot!\n",
        "                    for i in range(1,len(self.outcomeTitle)):\n",
        "                        finalstr += \"; \" + self.outcomeTitle[i]\n",
        "            #If the above fails, let's look in Design_Outcome(s)!\n",
        "            elif len(self.designMeasures) > 0:\n",
        "                finalstr = self.designMeasures[0]\n",
        "                if len(self.designMeasures) > 1: #nasty logic to get ; in right spot!\n",
        "                    for i in range(1,len(self.designMeasures)):\n",
        "                        finalstr += \"; \" + self.designMeasures[i]\n",
        "            return finalstr\n",
        "        def getTableBiomarkerOutcomeStr(self): #TODO, gotta figure out how to magically get these from primary outcomes... \n",
        "            #may be able to use the outcomes.descriptions to do this at some point\n",
        "            finalstr = \"\"\n",
        "            return finalstr\n",
        "        def generateTableRow(self, withEC=False): #Returns formatted list representing csv entry to fill out, leaves drug/match space blank\n",
        "            #The default parameter withEC can be set to True in call to print out extra column of NDD in Eligbility Criteria.\n",
        "            finalrow = [] #We will return a 1 dimensional list\n",
        "            finalrow.append(\"\") #column[0] is empty, I'll either keep empty for formatting or I'll add drug match if it's the first of a set\n",
        "            finalrow.append(self.nctid) #column[1] is NCTID  (this is all coming from Cross NDD worksheet Dr. Cummings sent as template)\n",
        "            finalrow.append(self.getShortPhase()) #column[2] is the Phase but shortened\n",
        "            finalrow.append(self.getConditionAcronymsStr()) #column[3] is the Diagnosis, AKA condition. \n",
        "            #Acronym isnt very reliable so instead used the condition name, but can revisit acronym at some point\n",
        "            if withEC: #This is optional column only added if withEC enabled\n",
        "                finalrow.append(self.getNDDInEligCriteriaStr())\n",
        "            finalrow.append(self.enrollment) #column[4] Number of participants\n",
        "            finalrow.append(self.getShortTimeFrame()) #column[5] Duration of Trial\n",
        "            finalrow.append(self.getTablePrimaryOutcomeStr()) #column[6] Primary Outcome(s)\n",
        "            finalrow.append(self.getTableBiomarkerOutcomeStr()) #column[7] Biomarker Outcome(s)\n",
        "            finalrow.append(self.getOnlyDateYear(self.firstpostedDate)) #column[8] Year Registered\n",
        "            finalrow.append(self.getComboStatus()) #column[9] Status (combo of last post date and actual status)\n",
        "            return finalrow\n",
        "        def getInterventionDrugsStr(self): #This tries to make a string with the drug name, for now just interventions, but \n",
        "            #we may eventually make this more sophisticated\n",
        "            finalstr = \"\"\n",
        "            if len(self.intervention) > 0:\n",
        "                finalstr = self.intervention[0]\n",
        "                if len(self.intervention) > 1: #nasty logic to get , in right spot!\n",
        "                    for i in range(1,len(self.intervention)):\n",
        "                        if jfc.drugnameCheckOK(self.intervention[i]): #False if placebo is in name\n",
        "                        #if self.intervention[i].lower() != 'placebo': #get rid of placebo\n",
        "                           finalstr += \", \" + self.intervention[i]\n",
        "            elif len(self.otherIntervention) > 0:\n",
        "                finalstr = self.otherIntervention[0]\n",
        "                if len(self.otherIntervention) > 1: #nasty logic to get , in right spot!\n",
        "                    for i in range(1,len(self.otherIntervention)):\n",
        "                        if jfc.drugnameCheckOK(self.otherIntervention[i]): #false if placebo in name\n",
        "                        #if self.otherIntervention[i].lower() != 'placebo': #get rid of placebo\n",
        "                           finalstr += \", \" + self.otherIntervention[i]\n",
        "            return finalstr\n",
        "        @staticmethod\n",
        "        def getOnlyDateYear(d):  #Static Function gets only the year from a date string (i.e: last posted date or first posted date)\n",
        "            return d[0:4]\n",
        "        def __str__(self):\n",
        "            printstr = \"NCTID: \" + self.nctid + \"\\nTitle: \" + self.title + \"\\nFirst Posted Date: \" \\\n",
        "            + self.firstpostedDate + \"\\nLast Posted Date: \" + self.lastpostedDate + \"\\nPhase: \" \\\n",
        "            + self.phase + \", Acronym: \" + self.acronym + \", Enrollment: \" \\\n",
        "            + self.enrollment + \"\\nTime Frame: \" + self.timeFrame + \"\\nCondition(s): \"  #The \\ is some python magic for multi-line i guess lol\n",
        "            for i in self.condition:\n",
        "                printstr += i + \"\\n\"\n",
        "            printstr += \"Intervention(s): \"\n",
        "            for i in self.intervention:\n",
        "                printstr += i + \"\\n\"\n",
        "            printstr += \"Intervention(s) Other Name(s): \"\n",
        "            for i in self.otherIntervention:\n",
        "                printstr += i + \"\\n\"\n",
        "            if len(self.outcomeTitle) > 0:\n",
        "                printstr += \"Outcome(s): \"\n",
        "                if len(self.outcomeTitle) != len(self.outcomeDescription):\n",
        "                    printstr += \"Warning Outcome Title / Descriptions do not match!\" \n",
        "                    printstr += str(len(self.outcomeTitle)) + \", \" + str(len(self.outcomeDescription)) + \"\\n\" \n",
        "                else:\n",
        "                    for i in range(len(self.outcomeTitle)):\n",
        "                        printstr += \"Title: \" + self.outcomeTitle[i] + \"\\n\"\n",
        "                        printstr += \"Description: \" + self.outcomeDescription[i] + \"\\n\"\n",
        "            else:\n",
        "                printstr += \"No Outcomes Listed.\\n\"\n",
        "            #Currently we do not print eligibility criteria, if we want to we can uncomment this to print them\n",
        "            #if len(self.eligibilityCriteria) > 0:\n",
        "            #    printstr += \"Eligibility Criteria: \"\n",
        "            #    for e in self.eligibilityCriteria:\n",
        "            #        printstr += str(e)\n",
        "            if len(self.nddInEligCriteria) > 0: #For NDD in Eligibility Criteria\n",
        "                printstr += \"NDD Listed in Eligibility Criteria that do not appear in Condition(s):\\n\"\n",
        "                printstr += self.getNDDInEligCriteriaStr() + \"\\n\"\n",
        "            return printstr\n",
        "    #End clinicalTrial class\n",
        "    class drugClinicalTrialSet: #Class I use instead of a dictionary for Tables 1 and 3\n",
        "        def __init__(self, drugname):\n",
        "            self.drugname = drugname\n",
        "            self.clinicalTrialList = []\n",
        "        def insertClinicalTrial(self, newCT):\n",
        "            self.clinicalTrialList.append(newCT)\n",
        "        def __str__(self):\n",
        "            printstr = \"Drugname: \" + self.drugname + \"\\n\"\n",
        "            for ctrial in self.clinicalTrialList:\n",
        "                printstr += str(ctrial)\n",
        "            return printstr\n",
        "    @staticmethod\n",
        "    def cleanOutputFiles():\n",
        "        import os\n",
        "        try:\n",
        "            for f in os.scandir(\"output/json-tables\"):\n",
        "                os.remove(f.path)\n",
        "            for f in os.scandir(\"output/classified-tables\"):\n",
        "                os.remove(f.path)\n",
        "            for f in os.scandir(\"output/final-tables/hyperlinked\"):\n",
        "                os.remove(f.path)\n",
        "            for f in os.scandir(\"output/final-tables\"):\n",
        "                if not f.is_dir(): #Don't try to delete directories or this explodes\n",
        "                    os.remove(f.path)\n",
        "        except:\n",
        "            print(\"Warning: Issues Cleaning up output directory before starting run\")\n",
        "    @staticmethod\n",
        "    def writeListToFileSorted(dataToWrite, fdir, fname): #Small function to write to a file a list of sorted items\n",
        "        sortedData = sorted(dataToWrite)\n",
        "        f = open(fdir + fname + \".txt\", \"w\")\n",
        "        for entry in sortedData:\n",
        "            f.write(entry)\n",
        "            f.write(\"\\n\")\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "################## START FUNCTIONS ################## START FUNCTIONS ################## START FUNCTIONS ##################\n",
        "\n",
        "#Function that is same as generateTable2 but instead makes a list of the  CTO objects (good for further processing)\n",
        "def generateCTOTableST(trialNCTIDlist, CTO): #generates list of objects\n",
        "    tableAsCTO = []\n",
        "    for nctid in trialNCTIDlist:\n",
        "        #For this table each entry only has 1 nctid\n",
        "        try:\n",
        "            newCTOSet = jfc.drugClinicalTrialSet(CTO[nctid[0]].getInterventionDrugsStr()) #create new set, give drug name\n",
        "            #For now we are just adding all drugs in there yolo, but we will want to maybe pick 1 later\n",
        "            newCTOSet.insertClinicalTrial(CTO[nctid[0]])\n",
        "        except KeyError:\n",
        "            print(\"CTO Hashing Error with NCTID:\", nctid[0])\n",
        "        tableAsCTO.append(newCTOSet)\n",
        "    return tableAsCTO\n",
        "\n",
        "#iTML: [Matched Drugname, List of NCTID of Trials Matching, Most recent last posted date of the set of trials]\n",
        "#Function that generates our final table as objects. Which can then be fed to generateTableFromCTOs to generate any table.\n",
        "#By doing this we generalize all data which is good\n",
        "def generateCTOTableIT(iTML, CTO): #generates list of objects\n",
        "    tableAsCTO = []\n",
        "    for entry in iTML:\n",
        "        newCTOSet = jfc.drugClinicalTrialSet(entry[0]) #create new set, give drug name\n",
        "        for curr_nctid in entry[1]: #now for the rest of the rows of this set\n",
        "            try:\n",
        "                newCTOSet.insertClinicalTrial(CTO[curr_nctid])\n",
        "            except KeyError:\n",
        "                print(\"Hashing Error with NCTID:\", curr_nctid)\n",
        "        tableAsCTO.append(newCTOSet) #add set to list\n",
        "    return tableAsCTO\n",
        "\n",
        "#This puppy can generate any table from our generic CTOs class\n",
        "def generateTableFromCTOs(tableTitle, CTOs):\n",
        "    columnTitles = ['Drug', 'Trial', 'Phase', 'Diagnosis','Number of Participants', \n",
        "                    'Duration of Trial', 'Primary Outcome(s)', 'Biomarker Outcome(s)', 'Year Registered', 'Status']\n",
        "    FinalTable = [tableTitle, columnTitles]    \n",
        "    for entry in CTOs:\n",
        "        for i in range(len(entry.clinicalTrialList)):\n",
        "            FinalTable.append(entry.clinicalTrialList[i].generateTableRow()) #create first row of set\n",
        "            if i == 0: \n",
        "                FinalTable[-1][0] = entry.drugname #Save Matched Drug in first row\n",
        "    return FinalTable\n",
        "\n",
        "#Modified pupper that has an extra column for Additional NDD in Eligibility Criteria\n",
        "def generateTableFromCTOsWithEC(tableTitle, CTOs):\n",
        "    columnTitles = ['Drug', 'Trial', 'Phase', 'Diagnosis', 'New NDD in Eligibility Criteria' ,'Number of Participants', \n",
        "                    'Duration of Trial', 'Primary Outcome(s)', 'Biomarker Outcome(s)', 'Year Registered', 'Status']\n",
        "    FinalTable = [tableTitle, columnTitles]    \n",
        "    for entry in CTOs:\n",
        "        for i in range(len(entry.clinicalTrialList)):\n",
        "            FinalTable.append(entry.clinicalTrialList[i].generateTableRow(withEC=True)) #create first row of set\n",
        "            if i == 0: \n",
        "                FinalTable[-1][0] = entry.drugname #Save Matched Drug in first row\n",
        "    return FinalTable\n",
        "\n",
        "def createCSVfromTable(tableName, fname):\n",
        "    with open('output/' + fname + '.csv', 'w', newline='') as csv_outfile:\n",
        "        outfile = csv.writer(csv_outfile)    \n",
        "        outfile.writerows(tableName)\n",
        "\n",
        "\n",
        "def createHyperLinkedCSV(fdir, fname): # Hyperlinked version of NDDCrossTableX.csv DO NOT ADD .CSV AT END\n",
        "    with open(fdir+fname+'.csv') as csv_file:\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        with open(fdir+\"hyperlinked/\"+fname+'HyperLinked.csv', 'w', newline='') as csv_outfile:\n",
        "            outfile = csv.writer(csv_outfile)\n",
        "            skipFirst2Rows = 2\n",
        "            for row in csv_data:\n",
        "                if skipFirst2Rows == 0:\n",
        "                    row[1] = \"=HYPERLINK(\\\"https://www.clinicaltrials.gov/ct2/show/\"+row[1]+\"\\\", \\\"\"+row[1]+\"\\\")\"\n",
        "                else:\n",
        "                    skipFirst2Rows -= 1\n",
        "                outfile.writerow(row)\n",
        "\n",
        "def makeJSONmatchedCTO(matchedCTO):\n",
        "    #Next let's generate and export the JSON of the dictionary to a file\n",
        "    #Note: right now we are just testing feasability of JSON and then importation into the app\n",
        "    #      Eventually what we will want to export are objects that are matching or other\n",
        "    #      stuff we will deal with.\n",
        "    #jsonmatchedCTO = json.dumps(matchedCTO[\"NCT00455143\"].__dict__, indent = 4)\n",
        "    jsonmatchedCTO = \"{\\n\"\n",
        "    for x in matchedCTO:\n",
        "        jsonmatchedCTO += \"\\\"\" + x + \"\\\" : \" + json.dumps(matchedCTO[x].__dict__, indent = 4) + \",\\n\"\n",
        "    jsonmatchedCTO = jsonmatchedCTO [:-2] + \"\\n}\"\n",
        "    #jsonmatchedCTO = \"{\\n\" + ',\\n'.join(str(\"\\\"\" + x + \"\\\" : \" + json.dumps(matchedCTO[x].__dict__, indent = 4)) for x in matchedCTO) + \"\\n}\"\n",
        "    #one-liner of the above 4 lines of code. Could be made even better with more joins but seems to be irrelevant for this size data\n",
        "    jsonFile = open(\"output/json-tables/jsonMatchedCTO.json\", \"w\")\n",
        "    jsonFile.write(jsonmatchedCTO)\n",
        "    jsonFile.close()\n",
        "\n",
        "#Function to generate a JSON from a list of CTOset objects, essentially to generate Final Tables in JSON Format\n",
        "def makeJSONFromCTOsList(CTOs, tablenum):\n",
        "    jsonCTOs = \"{\\n\"\n",
        "    for entry in CTOs:\n",
        "        jsonCTOs += \"\\\"\" + entry.drugname + \"\\\": [\\n\" \n",
        "        for ctrial in entry.clinicalTrialList:\n",
        "            jsonCTOs += json.dumps(ctrial.__dict__, indent = 4) + \",\\n\"\n",
        "        jsonCTOs = jsonCTOs[:-2] + \"\\n],\\n\"\n",
        "    jsonCTOs = jsonCTOs[:-2] + \"\\n}\\n\"\n",
        "    jsonFile = open(\"output/json-tables/jsonCTOTable\" + str(tablenum) + \".json\", \"w\")\n",
        "    jsonFile.write(jsonCTOs)\n",
        "    jsonFile.close()\n",
        "\n",
        "################### END FUNCTIONS ################### END FUNCTIONS ################### END FUNCTIONS ###################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Table Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For table generation\n",
        "# from common import jfc\n",
        "\n",
        "\n",
        "\n",
        "#This entry will hold table headings and we can add it to the top of the CSV file so it looks nice\n",
        "Table1Title   = [\"Table 1.  Drugs in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table2Title   = [\"Table 2.  Drugs in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table3Title   = [\"Table 3.  Biomarkers in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table4Title   = [\"Table 4.  Biomarkers in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table5Title   = [\"Table 5.  Device Interventions in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table6Title   = [\"Table 6.  Devices Interventions in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table7Title   = [\"Table 7.  Behavioral Interventions in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table8Title   = [\"Table 8.  Behavioral Interventions in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table9Title   = [\"Table 9.   Stem Cell Interventions in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table10Title  = [\"Table 10.  Stem Cell Interventions in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table11Title  = [\"Table 11.  Supplements as Interventions in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table12Title  = [\"Table 12.  Supplements as Interventions in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "#These are for reference only to make sure no trial that shouldn't have been deleted is listed here\n",
        "Table13Title  = [\"Table 13.  Deleted Intervention Matches in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table14Title  = [\"Table 14.  Deleted Intervention Matches in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "#These are for reference only to make sure no trials appear here. If they do then they need to be classified somewhere\n",
        "Table15Title  = [\"Table 15.  Unknown Intervention Class in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table16Title  = [\"Table 16.  Unknown Intervention Class in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table31Title  = [\"Table 31.  Unknown Intervention Subclass in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table32Title  = [\"Table 32.  Unknown Intervention Subclass in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "#These are for the subclasses\n",
        "Table17Title   = [\"Table 17.  Symptomatic Drugs in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table18Title   = [\"Table 18.  Symptomatic Drugs in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "Table19Title   = [\"Table 19.  Disease Modifying Drugs in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "Table20Title   = [\"Table 20.  Disease Modifying Drugs in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "\n",
        "#These are aggregate tables for reference only\n",
        "TableITTitle  = [\"Table IT.  All Intervention Matches in independent trials involving more than 1 neurodegenerative disorder.\"]\n",
        "TableSTTitle  = [\"Table ST.  All Intervention Matches in single trials that included more than 1 neurodegenerative disorder.\"]\n",
        "TableSTEC1Title  = [\"Table STEC1.  All Intervention Matches in single trials that included more than 1 neurodegenerative \" +\n",
        "                    \"disorder by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                    \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "TableSTEC2Title  = [\"Table STEC2.  All Intervention Matches in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                    \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "\n",
        "#New Tables for Eligibility Criteria Matches\n",
        "Table21Title   = [\"Table 21.  Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                  \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "Table22Title   = [\"Table 22.  Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "Table23Title   = [\"Table 23.  Non-Drug Interventions in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                  \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "Table24Title   = [\"Table 24.  Non-Drug Interventions in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "#Subclasses for Eligibility Criteria Matches:\n",
        "Table25Title   = [\"Table 25.  Symptomatic Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                  \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "Table26Title   = [\"Table 26.  Disease Modifying Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                  \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "Table27Title   = [\"Table 27.  Symptomatic Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "Table28Title   = [\"Table 28.  Disease Modifying Drugs in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "Table29Title   = [\"Table 29.  Unknown Intervention Subclass in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having at least one NDD listed as a Condition and by having one or more NDD listed in the \" +\n",
        "                  \"Eligibility Criteria that do not appear in the Conditions.\"]\n",
        "Table30Title   = [\"Table 30.  Unknown Intervention Subclass in single trials that included more than 1 neurodegenerative disorder \" +\n",
        "                  \"by having no NDD listed as Condition, but having one or more NDD listed in the Eligibility Criteria.\"]\n",
        "## not sure if I need me code in here or not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#eligcritprocesser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This file was born out of necessity. I need to run a lot of my existing code, but modified, to process these trials where\n",
        "#the eligibility criteria contains a Neurodegenerative Disease (NDD) meaning the subjects of the trial may have that NDD.\n",
        "#Because of this, these trials are of interest and I have created these csv files containing a list of NDD the trials \n",
        "#mentioned, the 3 files created contain either \n",
        "# 1) a single NDD detected that we then found was a new NDD in an existing trial with another NDD\n",
        "# 2) multiple NDD detected that we then found at least one was new compared to existing NDD listed as condition in a trial\n",
        "#    we were already tracking as of interest (since it had a NDD listed in conditions)\n",
        "# 3) multple NDD detected in a trial that had no NDD listed as conditions. Truly a new trial discovered. This one needs us\n",
        "#    to pull data from our raw files so we can get all the trial info.\n",
        "\n",
        "# For all three of these I need to classify the interventions listed to generate new tables (or append to existing?)\n",
        "# For now my big focus is to get the drug ones, but will expand to all intervention types. All that code I wrote/is done\n",
        "# for the other NDD trials, I just have to adapt it so it works here without crashing :D\n",
        "#TODO\n",
        "\n",
        "#Imports\n",
        "import tablegeneration as jft\n",
        "from common import jfc\n",
        "\n",
        "#Parameters: These are 1,2,and3 from above comment. The fourth one is out matchedCTO objects useful for 1,2 checks.\n",
        "#ec1matchedtrials: Stores NCTID, Trial Condition List, NDD listed in eligibility criteria text, and raw eligibility criteria text\n",
        "#ec2matchedtrials: Stores NCTID, Trial Condition List, NDD listed in eligibility criteria text,\n",
        "#                  New NDD appearing in Elig Crit Text, and raw eligibility criteria text\n",
        "#ecnewtrials:      Stores NCTID, NDD listed in eligibility criteria text, and raw eligibility criteria text\n",
        "#matchedCTOS:      Clinical Trial Objects. All NDD trials in a beautifully formatted way. Useful for 1 and 2 checks.\n",
        "def eligibilitycritprocessor(ec1matchedtrials, ec2matchedtrials, ecnewtrials, matchedCTO): \n",
        "    #Before anything remove list of trials to exclude due to manual checks\n",
        "    removeECexclusions(ec1matchedtrials, ec2matchedtrials, ecnewtrials)\n",
        "\n",
        "    #First let's build tableSTCTOs which we can use for 1 and 2. \n",
        "    tableSTCTOsEC1  = buildtableSTCTOs1(ec1matchedtrials,ec2matchedtrials, matchedCTO) #prepare to generate table\n",
        " \n",
        "    #Next let's get all the data (built CTO objects) for all the new trials which we need for 3\n",
        "    newtrialCTOs = buildECCTOs(ecnewtrials) #Will load output/eligibility-criteria/ csvs with trial data we need\n",
        "    tableSTCTOsEC2  = buildtableSTCTOs2(newtrialCTOs) #This prepares the data to generate the tables\n",
        "\n",
        "    #Save output to tables for both CTO groups:\n",
        "    tableSTEC1Final = jft.generateTableFromCTOsWithEC(jft.TableSTEC1Title, tableSTCTOsEC1) #Make Table\n",
        "    jft.createCSVfromTable(tableSTEC1Final, \"final-tables/NDDCrossTableSTEC1\") #Create CSV with table\n",
        "    jft.createHyperLinkedCSV(\"output/final-tables/\",\"NDDCrossTableSTEC1\") #Make hyperlinked version\n",
        "\n",
        "    tableSTEC2Final = jft.generateTableFromCTOsWithEC(jft.TableSTEC2Title, tableSTCTOsEC2) #Make Table\n",
        "    jft.createCSVfromTable(tableSTEC2Final, \"final-tables/NDDCrossTableSTEC2\") #Create CSV with table\n",
        "    jft.createHyperLinkedCSV(\"output/final-tables/\",\"NDDCrossTableSTEC2\") #Make hyperlinked version\n",
        "\n",
        "    return tableSTCTOsEC1, tableSTCTOsEC2 #Return it back so it can be fed to drugmatcher\n",
        "\n",
        "#Will open our elig-crit-exclusions.txt to get NCTID list of trials to remove from all 3\n",
        "#of our files since they are false positive matches. List was manually curated. So trust\n",
        "def removeECexclusions(ec1matchedtrials, ec2matchedtrials, ecnewtrials):    \n",
        "    exclusionSet = set()\n",
        "    f = open(\"input/classifiers/eligCritExcl.txt\", \"r\")\n",
        "    next(f) #skip first line\n",
        "    for line in f:\n",
        "        try:\n",
        "            nctid = line.partition(\";\")[0]\n",
        "            exclusionSet.add(nctid)\n",
        "        except:\n",
        "            print(\"Warning Could not Extract an NCTID from eligCritExcl.txt!\")\n",
        "    f.close()\n",
        "    #Fix: had to add [:] because we need to pass a copy of list and not list itself else it skips!\n",
        "    for entry in ec1matchedtrials[:]: #Remove trial\n",
        "        if entry[0] in exclusionSet:\n",
        "            ec1matchedtrials.remove(entry)\n",
        "    for entry in ec2matchedtrials[:]: #Remove trial\n",
        "        if entry[0] in exclusionSet:\n",
        "            ec2matchedtrials.remove(entry)\n",
        "    for entry in ecnewtrials[:]:      #Remove trial\n",
        "        if entry[0] in exclusionSet:\n",
        "            ecnewtrials.remove(entry)\n",
        "\n",
        "def buildtableSTCTOs1(ec1matchedtrials, ec2matchedtrials, matchedCTO):\n",
        "    matchedNCTIDset = set() #We need a list of NCTIDs of Trials of interest. Set so we remove dups\n",
        "    for ctrial in ec1matchedtrials:\n",
        "        matchedNCTIDset.add(ctrial[0]) #add NCTID, no duplicates allowed\n",
        "    for ctrial in ec2matchedtrials:\n",
        "        matchedNCTIDset.add(ctrial[0]) #add NCTID, no duplicates allowed \n",
        "    #Next we want to get the last posted date of all of these so we sort and store by that order.\n",
        "    matchedNCTIDList = [] #Will hold all NCTID of Trials we want, And also stores the last updated date\n",
        "    for ctrial in matchedNCTIDset:\n",
        "        matchedNCTIDList.append([matchedCTO[ctrial].nctid,matchedCTO[ctrial].lastpostedDate])\n",
        " \n",
        "    #Let's sort by post date so new stuff appears first (so descending order)\n",
        "    matchedNCTIDList.sort(key = lambda row: row[1], reverse=True)\n",
        "\n",
        "    #now that we have our list let's get our eligibility criteria version of Table 10 data basically.\n",
        "    ectableSTCTOs = jft.generateCTOTableST(matchedNCTIDList, matchedCTO)\n",
        "    return ectableSTCTOs\n",
        "\n",
        "#Functions serves as a way of using our old code for this new scenario. So it's like the bridge between\n",
        "#worlds \n",
        "def buildtableSTCTOs2(newtrialCTOs):\n",
        "    ecmatchedNCTIDList = [] #Will hold all NCTID of Trials we want, And also stores the last updated date\n",
        "    for ctrial in newtrialCTOs:\n",
        "        ecmatchedNCTIDList.append([newtrialCTOs[ctrial].nctid,newtrialCTOs[ctrial].lastpostedDate])\n",
        "     #Let's sort by post date so new stuff appears first (so descending order)\n",
        "    ecmatchedNCTIDList.sort(key = lambda row: row[1], reverse=True)\n",
        "    #now that we have our list let's get our eligibility criteria version of Table 10 data basically.\n",
        "    ectableSTCTOs = jft.generateCTOTableST(ecmatchedNCTIDList, newtrialCTOs)\n",
        "    return ectableSTCTOs\n",
        "\n",
        "#Function takes a set of trials NCTID that we want and then goes and gets all data from\n",
        "#raw files and then creates files with just the trials we want and saves them to the \n",
        "#output/eligibility-criterial folder with the ec prefix for further processing.\n",
        "#TL;DR: makes new csvs with trials of interest. Used in nddfilter to eligibility criteria trials\n",
        "def writeECTrialsfromNCTIDset(nctids): #Requires a set containing all NCTIDs we want\n",
        "    import csv #needed for function\n",
        "    ecmatchentries      = [] #stores matches\n",
        "    ecnddoutcomes       = [] #stores outcomes of matches\n",
        "    ecndddesignoutcomes = [] #Stores design outcomes of matches\n",
        "    with open(\"queries/chartsv4A.csv\") as csv_file: #Get Trial data\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            if row[0] in nctids: #row[0] has the studies.nct_id\n",
        "                ecmatchentries.append(row)\n",
        "    sorted_ecmatchentries = sorted(ecmatchentries, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "    with open('output/eligibility-criteria/ecnddtrials.csv', 'w', newline='') as csv_outfile:\n",
        "        outfile = csv.writer(csv_outfile)\n",
        "        outfile.writerows(sorted_ecmatchentries)\n",
        "\n",
        "    with open(\"queries/chartsv4B.csv\") as csv_file: #get outcome data\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            if row[0] in nctids: #this is from a trial we care about\n",
        "                    ecnddoutcomes.append(row) #save it\n",
        "    sorted_ecnddoutcomes = sorted(ecnddoutcomes, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "    with open('output/eligibility-criteria/ecnddoutcomes.csv', 'w', newline='') as csv_outfile:\n",
        "        outfile = csv.writer(csv_outfile)\n",
        "        outfile.writerows(sorted_ecnddoutcomes)\n",
        "\n",
        "    with open(\"queries/chartsv4C.csv\") as csv_file: #get design outcome data\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            if row[0] in nctids:\n",
        "                    ecndddesignoutcomes.append(row) #gg ez\n",
        "    sorted_ecndddesignoutcomes = sorted(ecndddesignoutcomes, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "    with open('output/eligibility-criteria/ecndddesignoutcomes.csv', 'w', newline='') as csv_outfile:\n",
        "        outfile = csv.writer(csv_outfile)\n",
        "        outfile.writerows(sorted_ecndddesignoutcomes)\n",
        "\n",
        "#The plan is to read in the eligibility-criteria data, \n",
        "#create our CTO objects from it, and return that at the end. Similar to what we did in\n",
        "#drugmatcherv6 with the original data. sounds ez right? Oh yeah and only create CTO\n",
        "#objects from NCTIDs found in the incoming parameter.\n",
        "#ecnewtrials: Stores NCTID, NDD listed in eligibility criteria text, and raw eligibility criteria text\n",
        "def buildECCTOs(ecnewtrials):\n",
        "    import csv #so we can load csv files\n",
        "    ecmatchentries      = [] #Store the rows of trial data we care about\n",
        "    ecnddoutcomes       = [] #Store the rows of outcomes we care about\n",
        "    ecndddesignoutcomes = [] #Stores the rows of design outcomes we care about\n",
        "    #Read in data\n",
        "    with open(\"output/eligibility-criteria/ecnddtrials.csv\") as csv_file:\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            ecmatchentries.append(row)\n",
        "\n",
        "    with open(\"output/eligibility-criteria/ecnddoutcomes.csv\") as csv_file:\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            ecnddoutcomes.append(row)\n",
        "\n",
        "    with open(\"output/eligibility-criteria/ecndddesignoutcomes.csv\") as csv_file:\n",
        "        csv_data = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_data:\n",
        "            ecndddesignoutcomes.append(row)\n",
        "\n",
        "    newNCTIDset = set() #Next let's make a set containing all the NCTIDs we want\n",
        "    for entry in ecnewtrials:\n",
        "        newNCTIDset.add(entry[0]) #entry[0] contains the NCTID, removes duplicates\n",
        "    #Let's try to build the CTO objects\n",
        "    ecmatchedCTO = {} #Matched Clinical Trial Object :D\n",
        "    currNCTID = \"NCT00000000\" #Code straight outa drugmatcher\n",
        "    for row in ecmatchentries:\n",
        "        if row[0] in newNCTIDset: #Relevant Trial, build CTO, otherwise we don't need it so ignore it.\n",
        "            if currNCTID != row[0]: #new NCTID\n",
        "                ecmatchedCTO[row[0]] = jfc.clinicalTrial(row[0],row[1],row[2],row[3],row[4],row[5],\n",
        "                             row[6],row[7],row[8],row[9],row[10],row[11], True) #new trial entry\n",
        "                #The true at end is because we are  setting condition to flexible\n",
        "                currNCTID = row[0]\n",
        "            else: #still inserting to current NCTID a new condition or intervention so handle it.\n",
        "                #Try adding condition, intervention, or intervention_other_names\n",
        "                ecmatchedCTO[row[0]].addCondition(row[2])\n",
        "                ecmatchedCTO[row[0]].addInterventions(row[3],0)\n",
        "                ecmatchedCTO[row[0]].addInterventions(row[4],1)\n",
        "    #Next let's go ahead and add the outcomes\n",
        "    for row in ecnddoutcomes:\n",
        "        if row[0] in newNCTIDset: #Relevant Trial, build CTO, otherwise we don't need it so ignore it.\n",
        "            ecmatchedCTO[row[0]].addOutcome(row[1],row[2])\n",
        "\n",
        "    #Next let's go ahead and add the design_outcomes (to see if we can fill more info for empty spots)\n",
        "    for row in ecndddesignoutcomes:\n",
        "        if row[0] in newNCTIDset: #Relevant Trial, build CTO, otherwise we don't need it so ignore it.\n",
        "            ecmatchedCTO[row[0]].addDesignOutcomes(row[1],row[2],row[3])\n",
        "\n",
        "    #So all objects are made now, so let's just add the eligibility criteria and NDD in elig criteria \n",
        "    for row in ecnewtrials:\n",
        "        if row[0] in ecmatchedCTO:\n",
        "            ecmatchedCTO[row[0]].addEligibilityCriteria(row[2]) #Add eligibilities to our objects\n",
        "            ecmatchedCTO[row[0]].addNDDInEligCriteria(list(row[1])) #Store NDD list\n",
        "        else:\n",
        "            print(\"Warning: Could not find NCTID of EC Trial in CTO List!\")\n",
        "\n",
        "    return ecmatchedCTO #Done\n",
        "\n",
        "#END CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Drug Classifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is the classification part of program\n",
        "# from common import jfc\n",
        "# import tablegeneration as jft\n",
        "\n",
        "\n",
        "#Function creates File with all drugname/interventions that appear in a given CTO set list. \n",
        "#Second parameter is name of file\n",
        "def printAllInterventions(CTOs, fname):\n",
        "    allInterventions = set()\n",
        "    for entry in CTOs:\n",
        "        allInterventions.add(entry.drugname)\n",
        "    #Now write to a file\n",
        "    sortedAllInterventions = sorted(allInterventions, key=None, reverse=False)\n",
        "    f = open(\"output/\" + fname + \".txt\", \"w\")\n",
        "    for entry in sortedAllInterventions:\n",
        "        f.write(entry)\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "    #At this point I can change function to return either a set: allInterventions or a sorted list: sortedAllInterventions\n",
        "    #return allInterventions, sortedAllInterventions #Should the need arise to work with either of them\n",
        "\n",
        "#functions to read in the input files containing what interventions belong to what class.\n",
        "def readFileAsSet(fname):\n",
        "    filecontents = set()\n",
        "    f = open(fname, \"r\")\n",
        "    for line in f:\n",
        "        line = line.strip('\\n')\n",
        "        if line != '':\n",
        "            filecontents.add(line.lower()) #Add everything lowercased (will be useful for string matching later)\n",
        "    f.close()\n",
        "    return filecontents\n",
        "\n",
        "def writeToFileSorted(data, fname):\n",
        "    sortedData = sorted(fname, key=None, reverse=False)\n",
        "    f = open(\"output/\" + fname + \".txt\", \"w\")\n",
        "    for entry in sortedData:\n",
        "        f.write(entry)\n",
        "        f.write(\"\\n\")\n",
        "    f.close()    \n",
        "\n",
        "def readClassifierFiles(): #NEW\n",
        "    #Will Hold: drugs, biomarkers, devices, behaviors, stemcells, supplements, to_delete (in that order)\n",
        "    drugClassifiers = {} #7 groups\n",
        "    drugClassifiers[\"drugs\"]       = readFileAsSet(\"input/classifiers/drugs-class.txt\")\n",
        "    drugClassifiers[\"biomarkers\"]  = readFileAsSet(\"input/classifiers/biomarkers-class.txt\")\n",
        "    drugClassifiers[\"devices\"]     = readFileAsSet(\"input/classifiers/devices-class.txt\")\n",
        "    drugClassifiers[\"behaviors\"]   = readFileAsSet(\"input/classifiers/behaviors-class.txt\")\n",
        "    drugClassifiers[\"stemcells\"]   = readFileAsSet(\"input/classifiers/stemcells-class.txt\")\n",
        "    drugClassifiers[\"supplements\"] = readFileAsSet(\"input/classifiers/supplement-class.txt\")\n",
        "    drugClassifiers[\"deleteList\"]  = readFileAsSet(\"input/classifiers/deleteList-class.txt\")\n",
        "\n",
        "    subdrugClassifiers = {} #2 groups:  Symptomatic Drugs and Disease Modifying Drugs\n",
        "    subdrugClassifiers[\"sx-drugs\"] = readFileAsSet(\"input/classifiers/sx-subclass.txt\")\n",
        "    subdrugClassifiers[\"dm-drugs\"] = readFileAsSet(\"input/classifiers/dm-subclass.txt\")   \n",
        "    return drugClassifiers, subdrugClassifiers\n",
        "\n",
        "def printClassifiedCTOs(CTOsList1, CTOsList2, CTOsList3, CTOsList4, subCTOsList1, subCTOsList2, subCTOsList3, subCTOsList4):\n",
        "    if len(CTOsList1) != 8 or len(CTOsList2) != 8 or len(CTOsList3) != 8 or len(CTOsList4) != 8:\n",
        "        print(\"Error: printClassifidCTOs: CTosLists not correct size\") #Eight Groups\n",
        "        return\n",
        "    printAllInterventions(CTOsList1[\"drugs\"]+CTOsList2[\"drugs\"]+\n",
        "                          CTOsList3[\"drugs\"]+CTOsList4[\"drugs\"],             \"classified-tables/drugs\")\n",
        "    printAllInterventions(CTOsList1[\"biomarkers\"]+CTOsList2[\"biomarkers\"]+\n",
        "                          CTOsList3[\"biomarkers\"]+CTOsList4[\"biomarkers\"],   \"classified-tables/biomarkers\")\n",
        "    printAllInterventions(CTOsList1[\"devices\"]+CTOsList2[\"devices\"]+\n",
        "                          CTOsList3[\"devices\"]+CTOsList4[\"devices\"],         \"classified-tables/devices\")\n",
        "    printAllInterventions(CTOsList1[\"behaviors\"]+CTOsList2[\"behaviors\"]+\n",
        "                          CTOsList3[\"behaviors\"]+CTOsList4[\"behaviors\"],     \"classified-tables/behaviors\")\n",
        "    printAllInterventions(CTOsList1[\"stemcells\"]+CTOsList2[\"stemcells\"]+\n",
        "                          CTOsList3[\"stemcells\"]+CTOsList4[\"stemcells\"],     \"classified-tables/stemcells\")\n",
        "    printAllInterventions(CTOsList1[\"supplements\"]+CTOsList2[\"supplements\"]+\n",
        "                          CTOsList3[\"supplements\"]+CTOsList4[\"supplements\"], \"classified-tables/supplements\")\n",
        "    printAllInterventions(CTOsList1[\"deleteList\"]+CTOsList2[\"deleteList\"]+\n",
        "                          CTOsList3[\"deleteList\"]+CTOsList4[\"deleteList\"],   \"classified-tables/deleteList\")\n",
        "    printAllInterventions(CTOsList1[\"unknownList\"]+CTOsList2[\"unknownList\"]+\n",
        "                          CTOsList3[\"unknownList\"]+CTOsList4[\"unknownList\"], \"classified-tables/unknownList\")\n",
        "\n",
        "    printAllInterventions(subCTOsList1[\"sx-drugs\"]+subCTOsList2[\"sx-drugs\"]+\n",
        "                          subCTOsList3[\"sx-drugs\"]+subCTOsList4[\"sx-drugs\"], \"classified-tables/sx-drugs\")\n",
        "    printAllInterventions(subCTOsList1[\"dm-drugs\"]+subCTOsList2[\"dm-drugs\"]+\n",
        "                          subCTOsList3[\"dm-drugs\"]+subCTOsList4[\"dm-drugs\"], \"classified-tables/dm-drugs\")\n",
        "    printAllInterventions(subCTOsList1[\"unknownList\"]+subCTOsList2[\"unknownList\"]+\n",
        "                          subCTOsList3[\"unknownList\"]+subCTOsList4[\"unknownList\"], \"classified-tables/unknownList-subclass\")\n",
        "\n",
        "def createFinalTables(CTOsListIT, CTOsListST, CTOsListSTEC1, CTOsListSTEC2, \n",
        "                      subCTOsListIT, subCTOsListST, subCTOsListSTEC1, subCTOsListSTEC2):\n",
        "    if len(CTOsListIT) != 8 or len(CTOsListST) != 8: #we want 4 groups\n",
        "        print(\"Error: createFinalTables:  CTosLists not correct size\")\n",
        "        return    \n",
        "    #Independent Trial Tables\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table1Title,  CTOsListIT[\"drugs\"]) ,       \"final-tables/NDDCrossTable1\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table3Title,  CTOsListIT[\"biomarkers\"]) ,  \"final-tables/NDDCrossTable3\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table5Title,  CTOsListIT[\"devices\"]) ,     \"final-tables/NDDCrossTable5\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table7Title,  CTOsListIT[\"behaviors\"]) ,   \"final-tables/NDDCrossTable7\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table9Title,  CTOsListIT[\"stemcells\"]) ,   \"final-tables/NDDCrossTable9\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table11Title, CTOsListIT[\"supplements\"]) , \"final-tables/NDDCrossTable11\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table13Title, CTOsListIT[\"deleteList\"]) ,  \"final-tables/NDDCrossTable13\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table15Title, CTOsListIT[\"unknownList\"]) , \"final-tables/NDDCrossTable15\")\n",
        "\n",
        "    #Single Trial Tables\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table2Title,  CTOsListST[\"drugs\"]) ,       \"final-tables/NDDCrossTable2\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table4Title,  CTOsListST[\"biomarkers\"]) ,  \"final-tables/NDDCrossTable4\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table6Title,  CTOsListST[\"devices\"]) ,     \"final-tables/NDDCrossTable6\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table8Title,  CTOsListST[\"behaviors\"]) ,   \"final-tables/NDDCrossTable8\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table10Title, CTOsListST[\"stemcells\"]) ,   \"final-tables/NDDCrossTable10\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table12Title, CTOsListST[\"supplements\"]) , \"final-tables/NDDCrossTable12\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table14Title, CTOsListST[\"deleteList\"]) ,  \"final-tables/NDDCrossTable14\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table16Title, CTOsListST[\"unknownList\"]) , \"final-tables/NDDCrossTable16\")\n",
        "\n",
        "    #Subclassification Tables Independent Trials\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table17Title, subCTOsListIT[\"sx-drugs\"]) ,    \"final-tables/NDDCrossTable17\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table19Title, subCTOsListIT[\"dm-drugs\"]) ,    \"final-tables/NDDCrossTable19\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table31Title, subCTOsListIT[\"unknownList\"]) , \"final-tables/NDDCrossTable31\")    \n",
        "    #Subclassification Tables Single Trials\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table18Title, subCTOsListST[\"sx-drugs\"]) ,    \"final-tables/NDDCrossTable18\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table20Title, subCTOsListST[\"dm-drugs\"]) ,    \"final-tables/NDDCrossTable20\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOs(jft.Table32Title, subCTOsListST[\"unknownList\"]) , \"final-tables/NDDCrossTable32\")\n",
        "\n",
        "    #Eligibility Criteria Flagged Tables:\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table21Title, CTOsListSTEC1[\"drugs\"]) ,          \"final-tables/NDDCrossTable21\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table22Title, CTOsListSTEC2[\"drugs\"]) ,          \"final-tables/NDDCrossTable22\")\n",
        "    nondrugsEC1 = (CTOsListSTEC1[\"biomarkers\"]+CTOsListSTEC1[\"devices\"]+CTOsListSTEC1[\"behaviors\"]\n",
        "                  +CTOsListSTEC1[\"stemcells\"]+CTOsListSTEC1[\"supplements\"]+CTOsListSTEC1[\"deleteList\"]+CTOsListSTEC1[\"unknownList\"])\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table23Title, nondrugsEC1) ,                     \"final-tables/NDDCrossTable23\")\n",
        "    nondrugsEC2 = (CTOsListSTEC2[\"biomarkers\"]+CTOsListSTEC2[\"devices\"]+CTOsListSTEC2[\"behaviors\"]\n",
        "                  +CTOsListSTEC2[\"stemcells\"]+CTOsListSTEC2[\"supplements\"]+CTOsListSTEC2[\"deleteList\"]+CTOsListSTEC2[\"unknownList\"])\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table24Title, nondrugsEC2) ,                     \"final-tables/NDDCrossTable24\")\n",
        "    #Subclassification for Eligibility Criteria Flagged Trials\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table25Title, subCTOsListSTEC1[\"sx-drugs\"]) ,    \"final-tables/NDDCrossTable25\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table26Title, subCTOsListSTEC1[\"dm-drugs\"]) ,    \"final-tables/NDDCrossTable26\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table29Title, subCTOsListSTEC1[\"unknownList\"]) , \"final-tables/NDDCrossTable29\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table27Title, subCTOsListSTEC2[\"sx-drugs\"]) ,    \"final-tables/NDDCrossTable27\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table28Title, subCTOsListSTEC2[\"dm-drugs\"]) ,    \"final-tables/NDDCrossTable28\")\n",
        "    jft.createCSVfromTable( jft.generateTableFromCTOsWithEC(jft.Table30Title, subCTOsListSTEC2[\"unknownList\"]) , \"final-tables/NDDCrossTable30\")\n",
        "\n",
        "    #Create Hyperlinked Tables:\n",
        "    for i in range(1,32+1): #32 because that's how many tables there are so far\n",
        "        jft.createHyperLinkedCSV(\"output/final-tables/\", \"NDDCrossTable\"+str(i))\n",
        "\n",
        "#Function classifies all CTOs into independent groups and returns a list of CTOs\n",
        "def classifyCTOs(CTOs, drugClassifiers, subClassifiers):\n",
        "    unclassified = set() #List of drugs that we couldn't classify\n",
        "    classifiedCTOs = {}\n",
        "    classifiedCTOs[\"drugs\"]       = []\n",
        "    classifiedCTOs[\"biomarkers\"]  = []\n",
        "    classifiedCTOs[\"devices\"]     = []\n",
        "    classifiedCTOs[\"behaviors\"]   = []\n",
        "    classifiedCTOs[\"stemcells\"]   = []\n",
        "    classifiedCTOs[\"supplements\"] = []\n",
        "    classifiedCTOs[\"deleteList\"]  = []\n",
        "    classifiedCTOs[\"unknownList\"] = []\n",
        "    #Subclassification of drugs\n",
        "    subclassifiedCTOs = {}\n",
        "    subclassifiedCTOs[\"sx-drugs\"]    = []\n",
        "    subclassifiedCTOs[\"dm-drugs\"]    = []\n",
        "    subclassifiedCTOs[\"unknownList\"] = []\n",
        "    #Time to Classify!\n",
        "    for entry in CTOs:\n",
        "        drugName = entry.drugname.lower() #Match lowercase version so it string matches\n",
        "        classified_success = False\n",
        "        #Check Each Category to see if the intervention is in it\n",
        "\n",
        "        #Can add more custom skipping here for drugnames\n",
        "\n",
        "        #First let's find exact matches.\n",
        "        #for i in range(len(drugClassifiers)):\n",
        "        for i in drugClassifiers: #Iterate through all the keys\n",
        "            if drugName in drugClassifiers[i]:\n",
        "                #We have matched to a class\n",
        "                classifiedCTOs[i].append(entry) #add CTO to this class\n",
        "                if i == \"drugs\": #If this CTO match is a drug, let's try to add to a drug subclass\n",
        "                    subclassified_success = False\n",
        "                    for j in subClassifiers:\n",
        "                        if drugName in subClassifiers[j]:\n",
        "                            subclassified_success = True #we matched a subclass\n",
        "                            subclassifiedCTOs[j].append(entry) #add CTO to this subclass too\n",
        "                    if not subclassified_success:\n",
        "                        subclassifiedCTOs[\"unknownList\"].append(entry)\n",
        "                classified_success = True\n",
        "                break #so we don't add same trial to 2 classes for now\n",
        "        #If we havent found a match it may be because this is a single trial multiple NDD table where\n",
        "        #The drugname is a list of all items. In which case we want to check each one separately\n",
        "        #Since they are comma separated we will go ahead and parse that.\n",
        "        if not classified_success:\n",
        "            splitdrugName = drugName.split(\", \")\n",
        "            if len(splitdrugName) > 1: #if true then this was one of those cases and we are now going to process it\n",
        "                #To process it we will run through each intervention and classify them individually,\n",
        "                #To decide what class we put the overall trial we will try several checks\n",
        "                #A - if one class has more interventions we group it to that (excluding deleteList class)\n",
        "                #B - If we have a tie we prioritize: drug > biom > device > behavior > stem > suppl. (exclude deleteList again)\n",
        "                #C  - Also we want to take out all deletList items from the string so those don't print on the list\n",
        "                #D  - if we still have no match then we just keep going to the other if statement\n",
        "                separatedInterventions = dict() #to store separate ones.\n",
        "                for dname in splitdrugName:\n",
        "                    for i in drugClassifiers: #Iterate through all the keys\n",
        "                        if dname in drugClassifiers[i]:\n",
        "                            #we found a match on the substring save it before doing master logic\n",
        "                            if i in separatedInterventions: #If key exists append\n",
        "                                separatedInterventions[i].append(dname)\n",
        "                            else: #make new one\n",
        "                                separatedInterventions[i] = [dname]\n",
        "                #Now we can do the logic mentioned above\n",
        "                \n",
        "                maxlength = 0\n",
        "                classcount = len(separatedInterventions)\n",
        "                #Find max length\n",
        "                for k in separatedInterventions:\n",
        "                    if len(separatedInterventions[k]) > maxlength:\n",
        "                        maxlength = len(separatedInterventions[k])\n",
        "                if maxlength > 0: #We found at least one match so keep going, otherwise give up, #Case #D\n",
        "                    finalclass = \"\"\n",
        "                    #Find all of the classes that got maxlength\n",
        "                    allmaxlengthclasses = []\n",
        "                    for k in separatedInterventions:\n",
        "                        if len(separatedInterventions[k]) == maxlength:\n",
        "                            allmaxlengthclasses.append(k)\n",
        "                    if classcount == 1: #only 1 class so just go with that regardless\n",
        "                        finalclass = allmaxlengthclasses[0] #no choice but to go with deletelist\n",
        "                    #From here on out it means we have multiple classes and have to make a choice... :\n",
        "                    #Multiple classes but there is one with more interventions, so go with that as long as it's not deleteList\n",
        "                    elif len(allmaxlengthclasses) == 1 and allmaxlengthclasses[0] != \"deleteList\": #case #A from above\n",
        "                        finalclass = allmaxlengthclasses[0]\n",
        "                    else: #Case #B Tie so prioritize\n",
        "                        if len(allmaxlengthclasses) == 1 and allmaxlengthclasses[0] == \"deleteList\":\n",
        "                            #the most common class was deleteclasses so before we go further let's find the next maxlength\n",
        "                            #and also update allmaxlengthclasses\n",
        "                            maxlength = 0\n",
        "                            allmaxlengthclasses = []\n",
        "                            for k in separatedInterventions:\n",
        "                                if len(separatedInterventions[k]) > maxlength and k != \"deleteList\":\n",
        "                                    maxlength = len(separatedInterventions[k])\n",
        "                            for k in separatedInterventions:\n",
        "                                if len(separatedInterventions[k]) == maxlength:\n",
        "                                    allmaxlengthclasses.append(k)\n",
        "                        #Okay now we can move on without worrying about that special case\n",
        "                        if len(allmaxlengthclasses) == 1:\n",
        "                            #the next maxlength only had one class, that's easy just pick that\n",
        "                            finalclass = allmaxlengthclasses[0]\n",
        "                        else: #we are here because it the current max length has more than 1 class\n",
        "                            #we got here either because deletelist was removed and next tier has a tie\n",
        "                            #or because the top winner has a tie and deleteList may be among those with tie\n",
        "                            #however if deletelist is here then there has to be another class too by the\n",
        "                            #logic written, and my logic is undeniable so do not worry about ending up\n",
        "                            #with having to pick deleteList, this should never happen at this point.\n",
        "                            #Priority: drug > biom > device > behavior > stem > suppl. (exclude deleteList again)\n",
        "                            #let's just do this with a bunch of if statements\n",
        "                            if \"drugs\" in allmaxlengthclasses:\n",
        "                                #Top priority assign this.\n",
        "                                finalclass = \"drugs\"\n",
        "                            elif \"biomarkers\" in allmaxlengthclasses:\n",
        "                                finalclass = \"biomarkers\"\n",
        "                            elif \"devices\" in allmaxlengthclasses:\n",
        "                                finalclass = \"devices\"\n",
        "                            elif \"behaviors\" in allmaxlengthclasses:\n",
        "                                finalclass = \"behaviors\"\n",
        "                            elif \"stemcells\" in allmaxlengthclasses:\n",
        "                                finalclass = \"stemcells\"\n",
        "                            elif \"supplements\" in allmaxlengthclasses:\n",
        "                                finalclass = \"supplements\"\n",
        "                            elif \"deleteList\" in allmaxlengthclasses:\n",
        "                                print(\"Warning: We shouldn't see deleteList Here, check code!\")\n",
        "                                finalclass = \"deleteList\"\n",
        "                            else:\n",
        "                                print(\"Warning: Unable to resolve final class!\")\n",
        "                    #Now that we have decided on a class, before appending, let's remove any \n",
        "                    #deleteList items from the drugname if we did not classify this to deleteList.\n",
        "                    if finalclass != \"deleteList\" and \"deleteList\" in separatedInterventions:\n",
        "                        splitOriginalDrugname = entry.drugname.split(\", \") #Current list of drugs\n",
        "                        newDrugName = \"\"\n",
        "                        #create new drugname, same as current but without deleteList interventions\n",
        "                        for i in splitOriginalDrugname:\n",
        "                            if i.lower() not in separatedInterventions[\"deleteList\"]:\n",
        "                                newDrugName += i + \", \"\n",
        "                        entry.drugname = newDrugName[:-2] #save the new drugname, remove last comma and space\n",
        "                    #Let's apppend!\n",
        "                    classifiedCTOs[finalclass].append(entry) #add CTO to this class\n",
        "                    classified_success = True\n",
        "\n",
        "                    if finalclass == \"drugs\": #If this finalclass is a drug, let's try to add to a drug subclass\n",
        "                        subclassified_success = False\n",
        "                        already_inserted_list = []\n",
        "                        for dname in splitdrugName:\n",
        "                            for i in subClassifiers: #Iterate through all the keys\n",
        "                                if i not in already_inserted_list and dname in subClassifiers[i]:\n",
        "                                    subclassified_success = True #we found a match on the substring \n",
        "                                    subclassifiedCTOs[i].append(entry)\n",
        "                                    already_inserted_list.append(i) #So we dont insert something twice if it has 2 sx\n",
        "                                    break\n",
        "                        if not subclassified_success:\n",
        "                            subclassifiedCTOs[\"unknownList\"].append(entry)\n",
        "\n",
        "        if not classified_success: #If we still haven't matched then let's try to search for partial match\n",
        "            #in the new version this  case really should not happen\n",
        "            #for i in range(len(drugClassifiers)): #check each class\n",
        "            for i in drugClassifiers: #check each class by key\n",
        "                for intervention in drugClassifiers[i]: #check each intervention in that class\n",
        "                    if intervention in drugName:\n",
        "                        classifiedCTOs[i].append(entry) #we found a partial match so let's go with that\n",
        "                        classified_success = True\n",
        "                        if i == \"drugs\":\n",
        "                            print(\"Verify:\", intervention+\":\", drugName)\n",
        "                            #let's try to add to a drug subclass\n",
        "                            subclassified_success = False\n",
        "                            already_inserted_list = []\n",
        "                            for dname in splitdrugName:\n",
        "                                for i in subClassifiers: #Iterate through all the keys\n",
        "                                    if i not in already_inserted_list and dname in subClassifiers[i]:\n",
        "                                        subclassified_success = True #we found a match on the substring \n",
        "                                        subclassifiedCTOs[i].append(entry)\n",
        "                                        already_inserted_list.append(i) #So we dont insert something twice if it has 2 sx\n",
        "                                        break\n",
        "                            if not subclassified_success:\n",
        "                                subclassifiedCTOs[\"unknownList\"].append(entry)\n",
        "                        break #so we stop checking for this drug\n",
        "                if classified_success: #If we found a match in previous class\n",
        "                    break #movie on to next CTO\n",
        "        if not classified_success: #if we still haven't found a match...\n",
        "            #let's just assume it's a non-drug, but also let's make a list of them for improving the system\n",
        "            classifiedCTOs[\"unknownList\"].append(entry)\n",
        "            unclassified.add(entry.drugname) #Append the normal casing\n",
        "    return classifiedCTOs, subclassifiedCTOs, unclassified\n",
        "\n",
        "\n",
        "#drugclassifier main function to separate intervention matched into groups like non-drug\n",
        "#disease-modifying drug, biomarkers, devices etc... for both CTOs: \n",
        "# CTOsIT (Independent Trials) and CTOsST (Single Trials)\n",
        "def runDrugClassifier(CTOsIT, CTOsST,  CTOsSTEC1, CTOsSTEC2):\n",
        "\n",
        "    #First let's save all interventions into a file and call it allinterventions\n",
        "    #This is pre removal of anything\n",
        "    printAllInterventions(CTOsIT+CTOsST+CTOsSTEC1+CTOsSTEC2, \"classified-tables/all-interventions\")\n",
        "\n",
        "    #next, load all text files to use to separate items\n",
        "    drugClassifiers, subclassifiers = readClassifierFiles()\n",
        "    \n",
        "    #Do classification and return a list of 3 CTOs sets classified\n",
        "    classifiedCTOsListIT, subclassifiedCTOsListIT, unclassified1 = classifyCTOs(CTOsIT, drugClassifiers, subclassifiers)\n",
        "    classifiedCTOsListST, subclassifiedCTOsListST, unclassified2 = classifyCTOs(CTOsST, drugClassifiers, subclassifiers)\n",
        "    classifiedCTOsListSTEC1, subclassifiedCTOsListSTEC1, unclassified3 = classifyCTOs(CTOsSTEC1, drugClassifiers, subclassifiers)\n",
        "    classifiedCTOsListSTEC2, subclassifiedCTOsListSTEC2, unclassified4 = classifyCTOs(CTOsSTEC2, drugClassifiers, subclassifiers)\n",
        "    unclassified = unclassified1.union(unclassified2).union(unclassified3).union(unclassified4)\n",
        "\n",
        "    #Create a list with all the unclassified so we can improve later\n",
        "    if len(unclassified1) != 0:\n",
        "        jfc.writeListToFileSorted(unclassified, \"output/classified-tables/\", \"unclassified-interventions\")\n",
        "\n",
        "    #create output files to be used for verification\n",
        "    printClassifiedCTOs(classifiedCTOsListIT, classifiedCTOsListST, classifiedCTOsListSTEC1, classifiedCTOsListSTEC2,\n",
        "                        subclassifiedCTOsListIT, subclassifiedCTOsListST, subclassifiedCTOsListSTEC1, subclassifiedCTOsListSTEC2)\n",
        "\n",
        "    #finally let's create our final tables as CSV\n",
        "    createFinalTables(classifiedCTOsListIT, classifiedCTOsListST, classifiedCTOsListSTEC1, classifiedCTOsListSTEC2,\n",
        "                      subclassifiedCTOsListIT, subclassifiedCTOsListST, subclassifiedCTOsListSTEC1, subclassifiedCTOsListSTEC2)\n",
        "\n",
        "#END CODE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Pre processing the query files to only handle the NDD trials we want for speeding up matching and not dealing with large files\n",
        "#Also sorts by NCTID resulting csvs (stores to output folder)\n",
        "#Current Runtime: 1 minute 36 seconds. \n",
        "\n",
        "import csv\n",
        "#import json #For JSON Export/Imports\n",
        "from common import jfc\n",
        "from eligcritprocesser import writeECTrialsfromNCTIDset\n",
        "\n",
        "# Row Contents\n",
        "# Row[0] studies.nct_id\n",
        "# Row[1] studies.official_title\n",
        "# Row[2] conditions.name\n",
        "# Row[3] interventions.name\n",
        "# Row[4] intervention_other_names.name\n",
        "# Row[5] studies.last_update_posted_date\n",
        "# Row[6] studies.phase\n",
        "# Row[7] studies.acronym\n",
        "# Row[8] studies.enrollment\n",
        "# Row[9] outcomes.time_frame\n",
        "# Row[10] studies.study_first_posted_date\n",
        "# Row[11] studies.overall_status\n",
        "\n",
        "\n",
        "matchentries = [] #store matches\n",
        "nddoutcomes = [] #stores nctid, outcome title/desc of relevant entries that match matchentries\n",
        "ndddesignoutcomes = [] #stores design outcomes nctid, measure, time frame, description (fills in missing stuff from normal outcomes)\n",
        "nddeligibilities = [] #stores nctid and elgibility criteria (for finding mentions of diseases that we are interested in)\n",
        "ignoretrials = set() #stores nctid of trials we want to ignore - used to ignore eligibility trials that are beyond time range we want\n",
        "trialsSaved = set() #stores nctid of all trials we currently identified as with a ndd\n",
        "with open(\"queries/chartsv4A.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        #Since we want time range 2010 to 2021 we are going to splice it from the string. Example 2018-06-23 we will take the\n",
        "        #2018 which is first 4 characters , cast to number, then compare if that value is in our range! else continue\n",
        "        if int(row[5][0:4]) < 2010 or int(row[5][0:4]) > 2021: #so 2009 or 2022 would not show up\n",
        "            ignoretrials.add(row[0])\n",
        "            continue #Remove these three lines to disable dating limits\n",
        "\n",
        "        toappend = False #becomes true when we find something with a matching NDD\n",
        "        #Check diseases in disease section\n",
        "        for disease in jfc.diseaseLongChecked: #check standard names\n",
        "            if disease in row[2]:\n",
        "                toappend = True\n",
        "                break\n",
        "        #Check acronyms\n",
        "        if (toappend == False) and (row[2] in jfc.diseaseAcronymsChecked):\n",
        "            toappend = True\n",
        "        #Removed row[7] acronym check because it was useless and got all false positives\n",
        "        #Check studies on healthy individuals that mention disease in the title of the trial.\n",
        "        if (toappend == False) and (\"Healthy Volunteers\" in row[2]):\n",
        "            for disease in jfc.diseaseinTitleChecked:\n",
        "                if disease in row[1]:\n",
        "                    toappend = True\n",
        "\n",
        "        #check lowercase or any title stuff we missed\n",
        "        if (toappend == False):\n",
        "            for disease in jfc.diseaselowercase:\n",
        "                if disease in row[2].lower():\n",
        "                    toappend = True\n",
        "                    break\n",
        "\n",
        "        if toappend:\n",
        "            matchentries.append(row)\n",
        "            trialsSaved.add(row[0]) #save this nctid as a trial we want to save in pass #2\n",
        "\n",
        "#first pass we get all NCTIDs we want, second pass we want to do a check of NCTID we skipped that have relevant titles\n",
        "#to see if a trial has a NDD title but no mention of NDD\n",
        "reviewskipped = [] #stores all trials we may want to review because they containe NDD in title, but not in condition\n",
        "multiplereview = {} #Has Trial and conditions matched. Used to then find the ones that have more than 1 condition\n",
        "multiplereviewfinal = [] #NDD trials that contain multiple NDD in title. Higher priority to review!\n",
        "with open(\"queries/chartsv4A.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        if row[0] not in trialsSaved and row[0] not in ignoretrials: #if we arent saving it and we arent ignoring it\n",
        "            for disease in jfc.diseaselowercase:\n",
        "                if disease in row[1].lower(): #to see if this catches anything\n",
        "                    reviewskipped.append([jfc.diseaseHashMap[disease],row[0],row[1],row[2],row[3],row[4]]) \n",
        "                    if row[0] in multiplereview:\n",
        "                        multiplereview[row[0]].add(jfc.diseaseHashMap[disease])\n",
        "                    else:\n",
        "                        multiplereview[row[0]] = set([jfc.diseaseHashMap[disease]])\n",
        "print(\"Trials skipped with NDD in title, but not in Condition:\", len(multiplereview)) #if we want to see skipped telemetry\n",
        "#search for any trials with multiple NDD in title\n",
        "for key in multiplereview:\n",
        "    if len(multiplereview[key]) > 1:\n",
        "        if len(multiplereview[key]) == 2 and 'MCI' in multiplereview[key] and 'AD' in multiplereview[key]:\n",
        "            continue #Skip MCI/AD pair\n",
        "        if len(multiplereview[key]) == 2 and 'MCI' in multiplereview[key] and 'PD' in multiplereview[key]:\n",
        "            continue #Skip MCI/PD pair\n",
        "        multiplereviewfinal.append([key, list(multiplereview[key])])\n",
        "if len(multiplereviewfinal) > 0:\n",
        "    reviewfinalmsg = \"Trials skipped we may want to review:\\n\"\n",
        "    for row in multiplereviewfinal:\n",
        "        reviewfinalmsg += row[0]+\": \"+row[1][0]\n",
        "        for i in range(1,len(row[1])):\n",
        "            reviewfinalmsg += \", \"+row[1][i]\n",
        "        reviewfinalmsg += \"\\n\"\n",
        "    print(reviewfinalmsg) #If we want to see telemetry on any trials to review.\n",
        "\n",
        "sorted_reviewskipped = sorted(reviewskipped, key=lambda row: row[1], reverse=False) #sort by nctid\n",
        "#Write skipped trials to review\n",
        "with open('output/nddreviewskipped.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(sorted_reviewskipped)\n",
        "\n",
        "#print(matchentries)\n",
        "diseasechecked = []\n",
        "for entry in matchentries:\n",
        "    if not (entry[2] in diseasechecked):\n",
        "        diseasechecked.append(entry[2])\n",
        "diseasechecked.sort()\n",
        "#Write all Disease Name Variations we matched to our NDD for verification purposes\n",
        "jfc.writeListToFileSorted(diseasechecked, \"output/\", \"ListofDiseaseNameMatched\")\n",
        "\n",
        "#next we sort the resulting list by NCTID\n",
        "sorted_matchentries = sorted(matchentries, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "\n",
        "#lets write to csv all matches for trials on these diseases to pass to the next file.\n",
        "#This script should only have to run once too.\n",
        "with open('output/nddtrials.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(sorted_matchentries)\n",
        "    #outfile.writerows(matchentries)\n",
        "\n",
        "#now let's add all of the outcome rows that match trials we care about that are in nddtrials.csv (since rest we don't care)\n",
        "\n",
        "matchnctid = {\"NCT00000000\"} #to speed up search let's make a set with all NCTIDs we care about\n",
        "for row in matchentries:\n",
        "    if row[0] not in matchnctid:\n",
        "        matchnctid.add(row[0])\n",
        "\n",
        "with open(\"queries/chartsv4B.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        if row[0] in matchnctid: #compare NCTID of matchentry with the row in chartsv4B if match then this is from a trial we care about\n",
        "                nddoutcomes.append(row) #if we found a match in nddtrials we insert and move on to next row in chartsv4B\n",
        "\n",
        "#next we sort the resulting list by NCTID\n",
        "sorted_nddoutcomes = sorted(nddoutcomes, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "\n",
        "with open('output/nddoutcomes.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(sorted_nddoutcomes)\n",
        "\n",
        "#let's do the same with design_outcomes (chartsv4C)\n",
        "with open(\"queries/chartsv4C.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        if row[0] in matchnctid: #compare NCTID of matchentry with the row in chartsv4B if match then this is from a trial we care about\n",
        "                ndddesignoutcomes.append(row) #if we found a match in nddtrials we insert and move on to next row in chartsv4B\n",
        "\n",
        "#next we sort the resulting list by NCTID\n",
        "sorted_ndddesignoutcomes = sorted(ndddesignoutcomes, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "\n",
        "with open('output/ndddesignoutcomes.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(sorted_ndddesignoutcomes)\n",
        "\n",
        "#Next let's load the eligbility criteria for trials (chartsv4D)\n",
        "with open(\"queries/chartsv4D.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data: #row[0] is nctid, row[1] is eligibility criteria text (big text)\n",
        "        if row[0] in ignoretrials:\n",
        "            continue #skip trials we are ignoring because they are out of search parameters (time range)\n",
        "        toappend = False #becomes true when we find something we want to save\n",
        "        #we need to only look in inclusion criteria so let's remove all text after the phrase\n",
        "        #\"Exclusion Criteria:\" appears in our string.\n",
        "        row1clean = row[1].lower()\n",
        "        if \"inclusion/exclusion\" not in row1clean:\n",
        "            row1clean = row1clean.split('exclusion criteria',1)[0]\n",
        "        condlist = [] #store conditions we find here\n",
        "        #Check each disease in disease section\n",
        "        for disease in jfc.diseaseLongChecked: #check standard names\n",
        "            if disease in row1clean: #if the text of a disease shows up\n",
        "                condlist.append(disease)\n",
        "                toappend = True\n",
        "                #break #Don't break so we can find more matches\n",
        "        #Check lowercase exceptions and some acronyms\n",
        "        if (toappend == False):\n",
        "            for disease in jfc.diseaselowercase:\n",
        "                if disease in row1clean.lower():\n",
        "                    condlist.append(disease)\n",
        "                    toappend = True\n",
        "        #Store the row if we flagged it as worthwhile\n",
        "        if toappend:\n",
        "            #Cleanup conditionlist.\n",
        "            clean_condlist = []\n",
        "            for c in condlist:\n",
        "                clean_condlist.append(jfc.diseaseHashMap[c])\n",
        "            nddeligibilities.append([row[0], len(condlist), ';'.join(clean_condlist) ,row[1] ])\n",
        "\n",
        "#sort the results by NCTID\n",
        "sorted_nddeligibilities = sorted(nddeligibilities, key=lambda row: row[0], reverse=False) #sort by nctid\n",
        "\n",
        "#Generate list of trials with 1  condition or with more than 1 condition listed\n",
        "nddeligibilities_multicond = []\n",
        "nddeligibilities_singlecond = []\n",
        "for row in sorted_nddeligibilities: \n",
        "    if row[1] > 1:\n",
        "        if row[1] == 2 and \"AD\" in row[2] and \"MCI\" in row[2]:\n",
        "                continue #2 conditions, skip if it is MCI/AD pair since we don't want it\n",
        "        if row[1] == 2 and \"PD\" in row[2] and \"MCI\" in row[2]:\n",
        "                continue #2 conditions, skip if it is MCI/PD pair since we don't want it    \n",
        "        nddeligibilities_multicond.append(row)\n",
        "    else:\n",
        "        nddeligibilities_singlecond.append(row)\n",
        "\n",
        "#write trials with eligibilities to review\n",
        "with open('output/eligibility-criteria/nddeligcritall.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(sorted_nddeligibilities)\n",
        "with open('output/eligibility-criteria/nddeligcritsinglecond.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(nddeligibilities_singlecond)\n",
        "with open('output/eligibility-criteria/nddeligcritmulticond.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(nddeligibilities_multicond)\n",
        "\n",
        "#Write All Trial Data for trials with eligibilities to review\n",
        "ecNCTIDset = set() #Set to store all the NCTIDs we want\n",
        "for entry in sorted_nddeligibilities:\n",
        "    ecNCTIDset.add(entry[0]) #Adds all NCTIDs we want and removes duplicates :D\n",
        "writeECTrialsfromNCTIDset(ecNCTIDset) #Run code to write all trial data to files.\n",
        "\n",
        "#END CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DrugMatcher V7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is where the program starts from\n",
        "from common import jfc\n",
        "import tablegeneration as jft\n",
        "import drugclassifier as jfd\n",
        "from eligcritprocesser import eligibilitycritprocessor\n",
        "\n",
        "import csv\n",
        "\n",
        "#This will take the nddtrials.csv and process from there (to speed it up)\n",
        "\n",
        "matchentries = []       #chartsv4A study and other main components\n",
        "nddoutcomes = []        #chartsv4B outcomes\n",
        "ndddesignoutcomes = []  #chartsv4C design outcomes\n",
        "nddeligsinglecond = []  #chartsv4D Trials with 1 NDD mention in the Eligibility Criteria\n",
        "nddeligmulticond  = []  #chartsv4D Trials with 2 or more NDD mention in the Eligibility Criteria\n",
        "\n",
        "finalentries = []\n",
        "\n",
        "#Cleanup Output Directory\n",
        "jfc.cleanOutputFiles() #Comment out for speed boost\n",
        "\n",
        "#Read in data\n",
        "with open(\"output/nddtrials.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        matchentries.append(row)\n",
        "\n",
        "with open(\"output/nddoutcomes.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        nddoutcomes.append(row)\n",
        "\n",
        "with open(\"output/ndddesignoutcomes.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        ndddesignoutcomes.append(row)\n",
        "\n",
        "with open(\"output/eligibility-criteria/nddeligcritsinglecond.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        nddeligsinglecond.append(row)\n",
        "\n",
        "with open(\"output/eligibility-criteria/nddeligcritmulticond.csv\") as csv_file:\n",
        "    csv_data = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_data:\n",
        "        nddeligmulticond.append(row)\n",
        "\n",
        "# Row Contents of output/nddtrials.csv\n",
        "# Row[0] studies.nct_id\n",
        "# Row[1] studies.official_title\n",
        "# Row[2] conditions.name\n",
        "# Row[3] interventions.name\n",
        "# Row[4] intervention_other_names.name\n",
        "# Row[5] studies.last_update_posted_date\n",
        "# Row[6] studies.phase\n",
        "# Row[7] studies.acronym\n",
        "# Row[8] studies.enrollment\n",
        "# Row[9] outcomes.time_frame\n",
        "# Row[10] studies.study_first_posted_date\n",
        "# Row[11] studies.overall_status\n",
        "\n",
        "# Row Contents of queries/chartsv4B.csv\n",
        "# Row[0] outcomes.nct_id \n",
        "# Row[1] outcomes.title\n",
        "# Row[2] outcomes.description\n",
        "\n",
        "# Row Contents of queries/chartsv4C.csv\n",
        "# Row[0] design_outcomes.nct_id \n",
        "# Row[1] design_outcomes.measure\n",
        "# Row[2] design_outcomes.time_frame\n",
        "# Row[3] design_outcomes.description\n",
        "\n",
        "# Row Contents of queries/chartsv4D.csv\n",
        "# Row[0] eligibilities.nct_id \n",
        "# Row[1] eligibilities.criteria\n",
        "\n",
        "# Row Contents of output/eligibility-criteria/nddeligcritsinglecond.csv and nddeligcritmulticond.csv\n",
        "# Row[0] eligibilities.nct_id \n",
        "# Row[1] Number of Identified NDD Acronyms\n",
        "# Row[2] NDD Acronym list separated by semi-colons.\n",
        "# Row[3] eligibilities.criteria (big text)\n",
        "\n",
        "#GOAL: \n",
        "#Drug Matched\n",
        "#NCT_ID\n",
        "#PHASE\n",
        "#Diagnosis\n",
        "#Number of Participants\n",
        "#Primary Outcome(s)\n",
        "#Biomarker Outcome(s)\n",
        "#Year Registered (first posted date)\n",
        "#Status (with date of last status, use last posted date)\n",
        "\n",
        "#NEW CODE FOR THE APP (TO MAKE JSON AND KEEP THIS AS OOP\n",
        "#newest cross ndd tables 4-19-21 on 5-14-21\n",
        "#Object Oriented Programming Time. Let's make our clinicalTrial objects using diseaseentrylist.csv\n",
        "matchedCTO = {} #Matched Clinical Trial Object :D\n",
        "TablesCTO  = [] #Stores All Table contents as Clinical Trial Objects along with matched drug strings (basically the same info as final tables)\n",
        "\n",
        "currNCTID = \"NCT00000000\"\n",
        "for row in matchentries:\n",
        "    if currNCTID != row[0]: #new NCTID\n",
        "        matchedCTO[row[0]] = jfc.clinicalTrial(row[0],row[1],row[2],row[3],row[4],row[5],row[6],row[7],row[8],row[9],row[10],row[11]) #new trial entry\n",
        "        currNCTID = row[0]\n",
        "    else: #still inserting to current NCTID a new condition or intervention so handle it.\n",
        "        #Try adding condition, intervention, or intervention_other_names\n",
        "        matchedCTO[row[0]].addCondition(row[2])\n",
        "        matchedCTO[row[0]].addInterventions(row[3],0)\n",
        "        matchedCTO[row[0]].addInterventions(row[4],1)\n",
        "#print('\\n'.join(str(matchedCTO[x]) for x in matchedCTO)) #Debug to Print the entire dictionary\n",
        "\n",
        "#Next let's go ahead and add the outcomes\n",
        "for row in nddoutcomes:\n",
        "    matchedCTO[row[0]].addOutcome(row[1],row[2])\n",
        "\n",
        "#Next let's go ahead and add the design_outcomes (to see if we can fill more info for empty spots)\n",
        "for row in ndddesignoutcomes:\n",
        "    matchedCTO[row[0]].addDesignOutcomes(row[1],row[2],row[3])\n",
        "\n",
        "#New Code for Eligibility Criteria\n",
        "#Next let's go ahead and add eligibility criteria for our trials and discover new trials worth looking at\n",
        "ec1matchedtrials = [] #Stores NCTID, Trial Condition List, NDD listed in eligibility criteria text, and raw eligibility criteria text\n",
        "ec2matchedtrials = [] #Stores NCTID, Trial Condition List, NDD listed in eligibility criteria text,\n",
        "                      #New NDD appearing in Elig Crit Text, and raw eligibility criteria text\n",
        "ecnewtrials = [] #Stores NCTID, NDD listed in eligibility criteria text, and raw eligibility criteria text\n",
        "\n",
        "#We need this here to add exclusion trials that we want to not add NDD for\n",
        "exclusionSet = set()\n",
        "f = open(\"input/classifiers/eligCritExcl.txt\", \"r\")\n",
        "next(f) #skip first line\n",
        "for line in f:\n",
        "    try:\n",
        "        nctid = line.partition(\";\")[0]\n",
        "        exclusionSet.add(nctid)\n",
        "    except:\n",
        "        print(\"Warning Could not Extract an NCTID from eligCritExcl.txt!\")\n",
        "f.close()\n",
        "\n",
        "#First for single NDD in Eligibility Criteria\n",
        "for row in nddeligsinglecond:\n",
        "    if row[0] in matchedCTO:   \n",
        "        matchedCTO[row[0]].addEligibilityCriteria(row[1]) #Add eligibilities to our objects in case we want to use it later\n",
        "        if row[0] in exclusionSet:\n",
        "            continue #skip this with manual skip\n",
        "        trialcondlist = set(matchedCTO[row[0]].getConditionAcronyms())\n",
        "        #If the eligibility criteria NDD we found is already listed in our trial conditions then we don't need to include this\n",
        "        if row[2] not in trialcondlist: #otherwise we do as we found a NDD that went as unlisted!\n",
        "            #Let's add one more check to exclude AD/MCI and PD/MCI pairs since we classify that as the same condition.\n",
        "            if row[2] == 'MCI' and ('AD' in trialcondlist or 'PD' in trialcondlist):\n",
        "                continue #Don't add it, skip it MCI/AD or MCI/PD pair\n",
        "            if row[2] == 'AD' and len(trialcondlist) == 1 and 'MCI' in trialcondlist:\n",
        "                continue #Don't add it, skip it. MCI/AD pair. Still may want to take a look at these later...\n",
        "            if row[2] == 'PD' and len(trialcondlist) == 1 and 'MCI' in trialcondlist:\n",
        "                continue #Don't add it, skip it. MCI/PD pair.            \n",
        "            matchedCTO[row[0]].addNDDInEligCriteria(row[2]) #Store NDD list for use later or for table printing\n",
        "            ec1matchedtrials.append([row[0], trialcondlist, row[2], str(row[3])]) #we will want to process further*\n",
        "    else:\n",
        "        #since we are only worried about single trials we don't need to process this further. However when we do independent trials we\n",
        "        #don't want to discard this trial just yet as we will want to look at the NDD listed and the intervention as a potential match\n",
        "        #candidate. For now we are good though. #TODO\n",
        "        pass\n",
        "\n",
        "#Next for multiple NDD listed in Eligility Criteria\n",
        "for row in nddeligmulticond:\n",
        "    if row[0] in matchedCTO:\n",
        "        if row[0] in exclusionSet:\n",
        "            continue #skip this with manual skip\n",
        "        matchedCTO[row[0]].addEligibilityCriteria(row[1]) #Add eligibilities to our objects in case we want to use it later (if not added already)\n",
        "        trialcondlist = set(matchedCTO[row[0]].getConditionAcronyms())\n",
        "        econdlist = set(row[2].split(';')) #split condition acronyms using the ; delimiter\n",
        "        #If all eligibility criteria text NDD we found are already listed in our trial conditions then we don't need to include this\n",
        "        if not econdlist.issubset(trialcondlist): #otherwise we do as we found a NDD that went as unlisted!  #Set theory is useful!\n",
        "            newndds = econdlist.symmetric_difference(trialcondlist) - trialcondlist #store the ones that are unseen for easier processing\n",
        "            if len(newndds) == 1  and 'MCI' in newndds and ('AD' in trialcondlist or 'PD' in trialcondlist):\n",
        "                continue #Don't add case where only new NDD is MCI and we already have PD or AD in trial\n",
        "            if len(newndds) == 1 and len(trialcondlist) == 1 and 'MCI' in trialcondlist and ('AD' in newndds or 'PD' in newndds):\n",
        "                continue #Don't add case where MCI is only Cond in Trial and we only have 1 extra new NDD (MCI/AD or MCI/PD)\n",
        "            matchedCTO[row[0]].addNDDInEligCriteria(list(newndds)) #Store NDD list for use later or for table printing\n",
        "            ec2matchedtrials.append([row[0], trialcondlist, econdlist, newndds, str(row[3])]) #we will want to process further*\n",
        "    else:\n",
        "        ecnewtrials.append([row[0], set(row[2].split(';')), row[3]]) #these are new trials not listed as NDD trials. Process further*\n",
        "\n",
        "#*further processing for ec1mmatchedtrials and ec2mmatchedtrials is to feed it through drugclassifier to see if \n",
        "# it's a trial with a dx or sm drug. For ecnewtrials we want to take these NCTIDs and see if any of them are for drugs. \n",
        "# But those I have to build CTOs first to be able to process. All of this I am going to do in the new file eligcritprocesser.py\n",
        "tableSTCTOsEC1, tableSTCTOsEC2 = eligibilitycritprocessor(ec1matchedtrials, ec2matchedtrials, ecnewtrials, matchedCTO)\n",
        "\n",
        "\n",
        "#write trials to review\n",
        "with open('output/eligibility-criteria/ec1matchedtrials.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(ec1matchedtrials)\n",
        "with open('output/eligibility-criteria/ec2matchedtrials.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(ec2matchedtrials)\n",
        "with open('output/eligibility-criteria/ecnewtrials.csv', 'w', newline='') as csv_outfile:\n",
        "    outfile = csv.writer(csv_outfile)\n",
        "    outfile.writerows(ecnewtrials)    \n",
        "#End New Code for Eligibility Critera\n",
        "\n",
        "#Test to see if this worked\n",
        "#print(matchedCTO[\"NCT00620191\"]) #For Testing the match from the table\n",
        "#print(matchedCTO[\"NCT00620191\"].generateTableRow())\n",
        "#print(matchedCTO[\"NCT04220021\"]) #For Testing the match from the table\n",
        "#print(matchedCTO[\"NCT04220021\"].generateTableRow())\n",
        "\n",
        "jft.makeJSONmatchedCTO(matchedCTO) #Not used currently but left on since may be useful to have this\n",
        "\n",
        "\n",
        "\n",
        "#TIME TO MAKE TABLES\n",
        "\n",
        "#Let's start with Table 2 of Cross NDD\n",
        "#    Table ST.  All interventions in single trials that included more than 1 neurodegenerative disorder.\n",
        "# This one is ez\n",
        "#we will just run through our object dictionary and find ones with length > 1 of the conditions array. Also good to test\n",
        "#all our system\n",
        "\n",
        "\n",
        "multiConditionNCTIDList = [] #Will hold all NCTID of Trials that have more than 1 condition listed, Am also going to store the last updated date\n",
        "#So I can then sort the trials by newest. This will make it look nice.\n",
        "#multiConditionNCTIDList.append(['NCT00000000', '2000-01-15']) #Test\n",
        "#print(matchedCTO[\"NCT03658135\"]) #For Testing the match from the table\n",
        "#print(matchedCTO[\"NCT03658135\"].generateTableRow())\n",
        "#print(matchedCTO[\"NCT03658135\"].getInterventionDrugsStr())\n",
        "#multiConditionNCTIDList.append(['NCT03658135', '2019-12-19']) #Test\n",
        "\n",
        "##TABLE ST MAIN CODE\n",
        "for ctrial in matchedCTO:\n",
        "    if (len(matchedCTO[ctrial].condition) > 1 and \n",
        "            matchedCTO[ctrial].condition[0] != \"Healthy Volunteers\" and \n",
        "            matchedCTO[ctrial].condition[1] != \"Healthy Volunteers\"): #Avoid detecting this false positive\n",
        "        #Found something to add. Save the NCTID and last posted date\n",
        "        if len(matchedCTO[ctrial].intervention) > 0 or len(matchedCTO[ctrial].otherIntervention) > 0: #don't add trials without drugs.\n",
        "            #Extra Special Check to avoid including MCI/AD pairs and PD/MCI pairs unless something else is involved.\n",
        "            try: \n",
        "                condlist = matchedCTO[ctrial].getConditionAcronyms()\n",
        "                if len(matchedCTO[ctrial].condition) == 2:\n",
        "                    if \"MCI\" in condlist and ( \"PD\" in condlist or \"AD\" in condlist):\n",
        "                        continue #Don'add!\n",
        "            except:\n",
        "                print(\"warning Filtering MCI/PD and MCI/AD! [1]\")\n",
        "                pass\n",
        "            #END NEW CODE CHECK\n",
        "            multiConditionNCTIDList.append([matchedCTO[ctrial].nctid,matchedCTO[ctrial].lastpostedDate])\n",
        "            #may be able to remove this if statement if we re-do our SQL query and remove non-interventional trials?\n",
        "\n",
        "#Let's sort by post date so new stuff appears first (so descending order)\n",
        "multiConditionNCTIDList.sort(key = lambda row: row[1], reverse=True)\n",
        "\n",
        "#Create our final version of Table 10\n",
        "tableSTCTOs = jft.generateCTOTableST(multiConditionNCTIDList, matchedCTO)\n",
        "tableSTFinal = jft.generateTableFromCTOs(jft.TableSTTitle, tableSTCTOs)\n",
        "\n",
        "#Write the new csv\n",
        "jft.createCSVfromTable(tableSTFinal, \"final-tables/NDDCrossTableST\")\n",
        "#with open('output/NDDCrossTableST.csv', 'w', newline='') as csv_outfile:\n",
        "#    outfile = csv.writer(csv_outfile)    \n",
        "#    outfile.writerows(tableSTFinal)\n",
        "\n",
        "\n",
        "#Create Hyperlinked version of NDDCrossTableST.csv\n",
        "jft.createHyperLinkedCSV(\"output/final-tables/\",\"NDDCrossTableST\")\n",
        "\n",
        "\n",
        "#TABLE IT MAIN CODE:\n",
        "#In this one we are looking for drugs that appear in more than 1 trial. First phase let's find exact match entries\n",
        "#Second phase later (with old code?) we can try to find partial matches by searching for individual words that match\n",
        "iTMLP1 = [] #Independent Trial Match List Phase 1, Each entry holds:\n",
        "# [  Matched Drugname, List of NCTID of Trials Matching (2 or more), Most recent last posted date of the set of trials  ]\n",
        "# The last one is so that I can sort by newest drugs or also by newest entries the individual dates is for sorting within a drug entry set.\n",
        "iTMLP2 = [] #Phase 2 of above.\n",
        "\n",
        "iTMDP1 = {} #Dictionary that we are going to hash into for Phase 1. All we need to store is the key: drug and value: nctid\n",
        "iTMDP2 = {} #Same for Phase 2. Stands for Independent Trial Match Dictonary Phase 2.\n",
        "\n",
        "#So my plan for this is to for first pass to simply try hashing the objects using intervention and intervention_other_name as keys\n",
        "#and then store for value a list of NCTID (so in case of collision we store them all). Store this in iTMDP1\n",
        "\n",
        "#When done we look through at any key/value pair that has more than 1 trial stored. Then we check to see if there is at least 2 trials\n",
        "#with different NDD. If so this is what we want so we add an entry into iTMLP1 so we can add it to our Table 1 results.\n",
        "\n",
        "#Then for the second pass we can try the same hash approach but by using individual words as keys but before we do that we want to \n",
        "#look at the word and see if it's a stopword so we can skip those. Other than that we do the same process.\n",
        "\n",
        "#Phase 1\n",
        "for ctrial in matchedCTO:\n",
        "    for drugname in matchedCTO[ctrial].intervention: \n",
        "        if jfc.drugnameCheckOK(drugname): #This is how we remove stopwords and other stuff we don't want like Placebo\n",
        "            #Check if key exists already\n",
        "            if drugname in iTMDP1: #Yes it exist, so append to existing list of NCTIDs\n",
        "                iTMDP1[drugname].append(matchedCTO[ctrial].nctid)\n",
        "            else: #New key so just add new entry\n",
        "                iTMDP1[drugname] = [matchedCTO[ctrial].nctid] #It's a list of NCTIDs, so we just insert a list with 1 NCTID.\n",
        "    for drugname in matchedCTO[ctrial].otherIntervention: #Do the same for intervention_other_names as above.\n",
        "        if jfc.drugnameCheckOK(drugname): #This is how we remove stopwords and other stuff we don't want like Placebo\n",
        "            if drugname in iTMDP1: \n",
        "                iTMDP1[drugname].append(matchedCTO[ctrial].nctid) #Same as above\n",
        "            else:\n",
        "                iTMDP1[drugname] = [matchedCTO[ctrial].nctid] #Same as above\n",
        "\n",
        "#Next let's look at iTMDP1 and find multiple trial drugs and see if there is at least 2 with different NDD listed.\n",
        "#Make sure we ignore the Healtyh Volunteers category.\n",
        "for drugname in iTMDP1:\n",
        "    alreadyAddedDrug = False\n",
        "    if len(iTMDP1[drugname]) > 1: #We have a drug with multiple NCTIDs so let's see if they are different NDD\n",
        "        try:\n",
        "            firstTrialConditions = matchedCTO[iTMDP1[drugname][0]].condition\n",
        "            for nctid in iTMDP1[drugname]:\n",
        "                if alreadyAddedDrug:\n",
        "                    break\n",
        "                for cond in matchedCTO[nctid].condition:\n",
        "                    if cond not in firstTrialConditions and cond != \"Healthy Volunteers\": #Don't count this as a different NDD\n",
        "                        #we found a trial in the set that has a different NDD, add drug entry to iTMLP1\n",
        "\n",
        "                        #TODO\n",
        "                        #Special Check to avoid including MCI/AD pairs and PD/MCI pairs unless something else is involved.\n",
        "                        #try: \n",
        "                        #    T1 = matchedCTO[iTMDP1[drugname][0]].getConditionAcronyms()\n",
        "                        #    T2 = matchedCTO[nctid].getConditionAcronyms()\n",
        "                        #    if len(T1) == 1 and len(T2) == 1:\n",
        "                        #        if \"MCI\" in T1 and (\"AD\" in T2 or \"PD\" in T2):\n",
        "                        #            continue \n",
        "                        #        if \"AD\"  in T1 and \"MCI\" in T2:\n",
        "                        #            continue\n",
        "                        #        if \"PD\"  in T1 and \"MCI\" in T2:\n",
        "                        #            continue\n",
        "                        #    if len(T1) == 2 and \"MCI\" in T1 and \"AD\" in T1:\n",
        "                        #        if len(T2) == 1 and \"AD\" in T2:\n",
        "                        #            continue\n",
        "                        #except:\n",
        "                        #    print(\"warning Filtering MCI/PD and MCI/AD! [2]\")\n",
        "                        #    pass\n",
        "                        #END NEW CODE CHECK                       \n",
        "                        #TODO\n",
        "\n",
        "                        setNCTIDsWDate = []\n",
        "                        for n in iTMDP1[drugname]:\n",
        "                            setNCTIDsWDate.append([n,matchedCTO[n].lastpostedDate]) #make a list holding all nctid and last posted date\n",
        "                        setNCTIDsWDate.sort(key=lambda col: col[1],reverse=True) #Sort Descending Order by Last Posted Date\n",
        "                        newestLastPostedDate = setNCTIDsWDate[0][1]\n",
        "                        #Find newest date to store that \n",
        "                        newentry = [drugname, [], newestLastPostedDate] #create entry\n",
        "                        for i in setNCTIDsWDate:\n",
        "                            newentry[1].append(i[0]) #Fill entry with only the nctids\n",
        "                        iTMLP1.append(newentry) #append it\n",
        "\n",
        "                        #Now that we added the set for this drugname let's move on to the next drugname by breaking to outermost loop\n",
        "                        alreadyAddedDrug = True\n",
        "                        break\n",
        "        except:\n",
        "            print(\"ERROR: Could not Find Cliical Trial in matchedCTO: \", nctid)\n",
        "    #else: #Nothing to see here\n",
        "\n",
        "\n",
        "#Let's sort by newest post date of each set so new stuff appears first (so descending order)\n",
        "iTMLP1.sort(key = lambda col: col[2], reverse=True)\n",
        "\n",
        "#Create our final version of Table IT\n",
        "tableITCTOs = jft.generateCTOTableIT(iTMLP1, matchedCTO)\n",
        "tableITFinal = jft.generateTableFromCTOs(jft.TableITTitle, tableITCTOs)\n",
        "\n",
        "#Write the new csv\n",
        "jft.createCSVfromTable(tableITFinal, \"final-tables/NDDCrossTableIT\")\n",
        "\n",
        "#Create Hyperlinked version of NDDCrossTableIT.csv\n",
        "jft.createHyperLinkedCSV(\"output/final-tables/\", \"NDDCrossTableIT\")\n",
        "\n",
        "#Phase 2 Here. TODO  (Single word matches)\n",
        "\n",
        "#Make JSONS of Table 1 and 2.  #These would be equivalent to table1Final once rendered as table\n",
        "jft.makeJSONFromCTOsList(tableITCTOs, 1)\n",
        "jft.makeJSONFromCTOsList(tableSTCTOs, 2)\n",
        "\n",
        "#Next we run drugclassifier to separate intervention matched into groups like non-drug, \n",
        "#disease-modifying drug, biomarkers, devices etc... for all CTOs  (ST, IT, and STECs)\n",
        "#IT is Independent Trials, ST is single trials, EC is Eligibility Criteria\n",
        "jfd.runDrugClassifier(tableITCTOs, tableSTCTOs, tableSTCTOsEC1, tableSTCTOsEC2) \n",
        "\n",
        "#END OF PROGRAM. CREATED BY JORGE FONSECA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##OLD STUFF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFtN6Ar64CgU"
      },
      "source": [
        "## tablegeneration.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1OCFpTNykMI"
      },
      "source": [
        "new code as Nov 26\n",
        "fixed erroe because new fun -  attribute 'diseaselowercase'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVjj_Y6tEhr7"
      },
      "source": [
        "# **Graphs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9XlNe2FEmtX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRc1aZtB-n74"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "rcParams['font.family'] = 'sans-serif'\n",
        "rcParams['font.sans-serif'] = ['Arial']\n",
        "rcParams['font.size'] = 16\n",
        "rcParams['axes.linewidth'] = 1.1\n",
        "rcParams['axes.labelpad'] = 10.0\n",
        "plot_color_cycle = cycler('color', ['000000', '0000FE', 'FE0000', '008001', 'FD8000', '8c564b', \n",
        "                                    'e377c2', '7f7f7f', 'bcbd22', '17becf'])\n",
        "rcParams['axes.prop_cycle'] = plot_color_cycle\n",
        "rcParams['axes.xmargin'] = 0\n",
        "rcParams['axes.ymargin'] = 0\n",
        "rcParams.update({\"figure.figsize\" : (12.8,4.8), #(6.8,4.8)\n",
        "                 \"figure.subplot.left\" : 0.177, \"figure.subplot.right\" : 0.946,\n",
        "                 \"figure.subplot.bottom\" : 0.156, \"figure.subplot.top\" : 0.965,\n",
        "                 \"axes.autolimit_mode\" : \"round_numbers\",\n",
        "                 \"xtick.major.size\"     : 7,#7\n",
        "                 \"xtick.minor.size\"     : 3.5,#3.5\n",
        "                 \"xtick.major.width\"    : 1.1,\n",
        "                 \"xtick.minor.width\"    : 1.1,\n",
        "                 \"xtick.major.pad\"      : 5,\n",
        "                 'xtick.labelsize'      : 12, #taamao X\n",
        "                 \"xtick.minor.visible\" : True,\n",
        "                 \"ytick.major.size\"     : 7,\n",
        "                 \"ytick.minor.size\"     : 3.5,\n",
        "                 \"ytick.major.width\"    : 1.1,\n",
        "                 \"ytick.minor.width\"    : 1.1,\n",
        "                 \"ytick.major.pad\"      : 5,\n",
        "                 \"ytick.minor.visible\" : True,\n",
        "                 \"lines.markersize\" : 10,\n",
        "                 \"lines.markerfacecolor\" : \"none\",\n",
        "                 \"lines.markeredgewidth\"  : 0.8})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVJxixnJ3liL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcg2oCgMV3rw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6WDlhwdRQM2"
      },
      "outputs": [],
      "source": [
        "def fun_barplot(name):\n",
        "  dtx = pd.read_csv('output/final-tables/hyperlinked/'+name+'.csv', header = 1)\n",
        "\n",
        "  stado_n=[]\n",
        "  for i in range(len(dtx['Status'])):\n",
        "    val=dtx['Status'][i].split(';')[1]\n",
        "    stado_n.append(val)\n",
        "  dtx['Status_new']=stado_n\n",
        "\n",
        "  fig1 = plt.figure()\n",
        "  dtx['Status_new'].value_counts().plot(kind='bar', color='g')\n",
        "  #plt.savefig(\"1.png\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.close(fig1)\n",
        "\n",
        "  fig2 = plt.figure()\n",
        "  dtx['Diagnosis'].value_counts().plot(kind='bar', color='b')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.close(fig2)\n",
        "\n",
        "  fig3 = plt.figure()\n",
        "  dtx['Phase'].value_counts().plot(kind='bar', color='r')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.close(fig3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # fig4 = plt.figure()\n",
        "  # dtx['Drug'].value_counts('Trails').plot(kind='bar', color='r')\n",
        "  # plt.legend()\n",
        "  # plt.show()\n",
        "  # plt.close(fig4)\n",
        "\n",
        "\n",
        "  fig5 = plt.figure()\n",
        "  dtx['Year Registered'].value_counts().plot(kind='bar', color='g')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.close(fig5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_f9fynSLzG"
      },
      "outputs": [],
      "source": [
        "fun_barplot('NDDCrossTable1HyperLinked')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgC4PYRXZ01l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0K99EbLaPSc"
      },
      "source": [
        "# Save OUTPUTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEkyNxTOqovh"
      },
      "outputs": [],
      "source": [
        "##save OUTPUT\n",
        "!rm -rfv /content/NDDDrugMatcher/output/OUTPUT.zip \n",
        "!zip -r /content/NDDDrugMatcher/output/OUTPUT.zip  /content/NDDDrugMatcher/./output/  -x content/NDDDrugMatcher/./queries/**\\* content/NDDDrugMatcher/./.git/**\\* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV2oWVrCm2Zx"
      },
      "outputs": [],
      "source": [
        "!ls -lsah /content/NDDDrugMatcher/output/OUTPUT.zip \n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKsDG5Cvqrcr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/NDDDrugMatcher/output/OUTPUT.zip') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7GYTdQ9pCrH"
      },
      "source": [
        "# Find\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkIWI6-ODAOk"
      },
      "source": [
        "## Data find"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g72O01VTJIuA"
      },
      "outputs": [],
      "source": [
        "# def fun_finder(letter):\n",
        "#   li = glob.glob(\"output/final-tables/*.csv\")\n",
        "#   list_fun=[]\n",
        "\n",
        "#   for i in range(len(li)):\n",
        "#       dfx = pd.read_csv(li[i], header = 1)\n",
        "#       dff=dfx.copy()\n",
        "#       for i in range(len(dff['Diagnosis'])):\n",
        "#         if dff['Diagnosis'][i].find(letter) != -1:\n",
        "#           list_fun.append(dff['Drug'][i])\n",
        "#           #print(dttt['Drug'][i])\n",
        "#         else:\n",
        "#           pass \n",
        "\n",
        "#   name = 'Drugs for '+letter\n",
        "#   datfarm=pd.DataFrame(np.array(list_farm),columns=[name])\n",
        "#   datfa=datfarm.dropna(axis=0, how ='any')\n",
        "#   datf=datfa.drop_duplicates(keep='last')\n",
        "#   dat=datf.reset_index(drop=True)\n",
        "#   print(dat)\n",
        "#   print('Amount of medication for '+letter+' :'+str(len(dat)-1))\n",
        "\n",
        "#   return dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTTs_L2xJbKQ"
      },
      "outputs": [],
      "source": [
        "# letter=input()\n",
        "# datfar = fun_find_fun(letter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Behl-9j7MusB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rpfZxUK513n0",
        "eVjj_Y6tEhr7",
        "R0K99EbLaPSc"
      ],
      "machine_shape": "hm",
      "name": "NDDDrugMatcher_ry_march_2022",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
